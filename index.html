<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://example.com/"/>





  <title>Hexo</title>
  








<meta name="generator" content="Hexo 6.3.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/09/15/VINSMONO-NOTE-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/09/15/VINSMONO-NOTE-4/" itemprop="url">VINS-MONO源码阅读:初始化2</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2023-09-15T16:25:16+08:00">
                2023-09-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="VINSMONO学习笔记-4"><a href="#VINSMONO学习笔记-4" class="headerlink" title="VINSMONO学习笔记(4)"></a>VINSMONO学习笔记(4)</h1><p>这篇博客继续学习初始化，还剩下visualInitialAlign()一个函数要看，但内容比较多，也非常重要，就全放在这篇博客里了。</p>
<hr>
<h2 id="原理讲解及源码阅读"><a href="#原理讲解及源码阅读" class="headerlink" title="原理讲解及源码阅读"></a>原理讲解及源码阅读</h2><h3 id="标定IMU角速度bias"><a href="#标定IMU角速度bias" class="headerlink" title="标定IMU角速度bias"></a>标定IMU角速度bias</h3><p><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/gb1.png" alt="g_b_1"><br>经过sfm步骤后，$q_{b_{k+1}}^{c_0}$和$q_{b_{k}}^{c_0}$都已得到，正常情况下，上面的式子肯定是单位四元数，但如果直接用之前预积分的值，由于没有减去角速度bias的值，所以上面的式子有误差，这里要优化$\gamma _{b_{k + 1} }^{b_k}$,给它加上一个bias值，经过优化的$\gamma _{b_{k + 1}}^{b_k}$可以通过一阶泰勒展开表示：<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/gb2.png" alt="g_b_2"><br>这里的$\hat \gamma _{b_{k + 1} }^{b_k}$就是之前预积分出来的结果，未考虑bias。讲上式代入误差公式，误差公式应该等于单位四元数，再将除bias的变量移到等式右边，得到下式：<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/bg3.png" alt="g_b_3"><br>这里只考虑虚部，再把$\hat \gamma _{b_{k + 1} }^{b_k}$移到右边可得到下式：<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/gb4.png" alt="g_b_4"><br>两边再乘以雅可比的转置，就可得到增量方程的形式了：<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/gb5.png" alt="g_b_5"><br>然后用ldlt分解求解这个增量方程就行。这里不需要迭代优化这个$\gamma _{b_{k + 1} }^{b_k}$，只要进行一次求解，得到这个bias就行。</p>
<p>下面看详细代码,先主要来看得到增量方程的那个循环:</p>
<pre><code>for (frame_i = all_image_frame.begin(); next(frame_i) != all_image_frame.end(); frame_i++)
&#123;
    frame_j = next(frame_i);
    MatrixXd tmp_A(3, 3);
    tmp_A.setZero();
    VectorXd tmp_b(3);
    tmp_b.setZero();
    Eigen::Quaterniond q_ij(frame_i-&gt;second.R.transpose() * frame_j-&gt;second.R);
    tmp_A = frame_j-&gt;second.pre_integration-&gt;jacobian.template block&lt;3, 3&gt;(O_R, O_BG);
    tmp_b = 2 * (frame_j-&gt;second.pre_integration-&gt;delta_q.inverse() * q_ij).vec();
    A += tmp_A.transpose() * tmp_A;
    b += tmp_A.transpose() * tmp_b;
&#125;
</code></pre><p>这里就是遍历所有图像帧，frame_i就是当前帧，frame_j就是当前帧的下一帧，q_ij就是通过图像帧旋转和之前标定的$q_{bc}$得到$b_{k+1}$到$b_{k}$的旋转，tmp_A是预积分$\gamma$对角速度bias的雅可比。之前预积分的时候对pre_integration的jacobian中的残差对$b_a,b_g$的雅可比进行了更新，这里从中取了我们需要的部分，具体可以去看预积分那篇最后雅可比的图，还是挺一目了然的。</p>
<p>使用ldlt分解求得bias后要重新进行预积分，把角速度bias带进去：</p>
<pre><code>for (frame_i = all_image_frame.begin(); next(frame_i) != all_image_frame.end( ); frame_i++)
&#123;
    frame_j = next(frame_i);
    frame_j-&gt;second.pre_integration-&gt;repropagate(Vector3d::Zero(), Bgs[0]);
&#125;
</code></pre><p>函数repropagate实现了重新求解预积分，如下所示：</p>
<pre><code>void repropagate(const Eigen::Vector3d &amp;_linearized_ba, const Eigen::Vector3d &amp;_linearized_bg)
&#123;
    sum_dt = 0.0;
    acc_0 = linearized_acc;
    gyr_0 = linearized_gyr;
    delta_p.setZero();
    delta_q.setIdentity();
    delta_v.setZero();
    linearized_ba = _linearized_ba;
    linearized_bg = _linearized_bg;
    jacobian.setIdentity();
    covariance.setZero();
    for (int i = 0; i &lt; static_cast&lt;int&gt;(dt_buf.size()); i++)
        propagate(dt_buf[i], acc_buf[i], gyr_buf[i]);
&#125;
</code></pre><p>这里就是先把预积分相关的变量全部先置为0，再遍历上一帧到这一帧的所有imu数据，这些都存在pre_integration对象的成员变量dt_buf, acc_buf, gyr_buf容器中，调用函数propagate()，按遍历顺序传入对应参数，重新进行预积分。</p>
<hr>
<h3 id="初始化速度、重力和尺度因子"><a href="#初始化速度、重力和尺度因子" class="headerlink" title="初始化速度、重力和尺度因子"></a>初始化速度、重力和尺度因子</h3><p>先确定要估计的变量：<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/gvs1.png" alt="gvs1"><br>其中，$v_k^{b_k}$表示$k$时刻body坐标系的速度在body坐标系下的表示。$g^{c_0}$为重力向量在参考帧(即之前的第l帧)相机坐标系下的表示。$s$表示尺度因子，将视觉轨迹拉伸到米制单位。</p>
<p>世界坐标系下的预积分约束如下：<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/gvs2.png" alt="gvs2"></p>
<p>但是$w$坐标系我们不知道，只知道l帧相机坐标系，所以需要把上面的公式转到l帧相机坐标系上(<strong>下面的公式中$c_0$其实是l帧</strong>)：<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/gvs3.png" alt="gvs3"></p>
<p>先看$\alpha_{b_kb_{k+1}}$,理想状态下，式子左边减去右边结果应该为0，下面看详细推导：<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/gvs5.png" alt="gvs4"></p>
<p>把要估计的量及其系数放在等式右边，其他的放在左边，再转换为矩阵形式，可得到下式：<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/gvs6.png" alt="gvs5"></p>
<p>接下来再看$\beta_{b_kb_{k+1}}$，具体操作和上面差不多：<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/gvs7.png" alt="gvs6"></p>
<p>然后将两个矩阵合并：<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/gvs8.png" alt="gvs7"></p>
<p>最后将其变成增量方程形式:<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/gvs9.png" alt="gvs8"><br>然后和标定角速度bias时一样，用ldlt分解求得估计值就行，下面看具体代码:</p>
<pre><code>bool LinearAlignment(map&lt;double, ImageFrame&gt; &amp;all_image_frame, Vector3d &amp;g, VectorXd &amp;x)
&#123;
    int all_frame_count = all_image_frame.size();
    int n_state = all_frame_count * 3 + 3 + 1;

    MatrixXd A&#123;n_state, n_state&#125;;
    A.setZero();
    VectorXd b&#123;n_state&#125;;
    b.setZero();

    map&lt;double, ImageFrame&gt;::iterator frame_i;
    map&lt;double, ImageFrame&gt;::iterator frame_j;
    int i = 0;
    ...
    ...
&#125;
</code></pre><p>这里是定义总的增量方程的左边的信息矩阵A以及右边的雅可比乘残差矩阵b，n_state是矩阵大小。上面原理部分只是讲了两帧之间的参数估计，但实际上，每加一帧就要多估计一个v。因此，下面计算增量方程时，都是按帧遍历，计算完两帧之间的增量方程的左右两边，再把数据放到A,b对应矩阵块里面，下面为具体代码：</p>
<pre><code>for (frame_i = all_image_frame.begin(); next(frame_i) != all_image_frame.end(); frame_i++, i++)
&#123;
    frame_j = next(frame_i);

    MatrixXd tmp_A(6, 10);
    tmp_A.setZero();
    VectorXd tmp_b(6);
    tmp_b.setZero();

    double dt = frame_j-&gt;second.pre_integration-&gt;sum_dt;

    tmp_A.block&lt;3, 3&gt;(0, 0) = -dt * Matrix3d::Identity();
    tmp_A.block&lt;3, 3&gt;(0, 6) = frame_i-&gt;second.R.transpose() * dt * dt / 2 * Matrix3d::Identity();
    tmp_A.block&lt;3, 1&gt;(0, 9) = frame_i-&gt;second.R.transpose() * (frame_j-&gt;second.T - frame_i-&gt;second.T) / 100.0;     
    tmp_b.block&lt;3, 1&gt;(0, 0) = frame_j-&gt;second.pre_integration-&gt;delta_p + frame_i-&gt;second.R.transpose() * frame_j-&gt;second.R * TIC[0] - TIC[0];
    //cout &lt;&lt; &quot;delta_p   &quot; &lt;&lt; frame_j-&gt;second.pre_integration-&gt;delta_p.transpose() &lt;&lt; endl;
    tmp_A.block&lt;3, 3&gt;(3, 0) = -Matrix3d::Identity();
    tmp_A.block&lt;3, 3&gt;(3, 3) = frame_i-&gt;second.R.transpose() * frame_j-&gt;second.R;
    tmp_A.block&lt;3, 3&gt;(3, 6) = frame_i-&gt;second.R.transpose() * dt * Matrix3d::Identity();
    tmp_b.block&lt;3, 1&gt;(3, 0) = frame_j-&gt;second.pre_integration-&gt;delta_v;
    //cout &lt;&lt; &quot;delta_v   &quot; &lt;&lt; frame_j-&gt;second.pre_integration-&gt;delta_v.transpose() &lt;&lt; endl;

    Matrix&lt;double, 6, 6&gt; cov_inv = Matrix&lt;double, 6, 6&gt;::Zero();
    //cov.block&lt;6, 6&gt;(0, 0) = IMU_cov[i + 1];
    //MatrixXd cov_inv = cov.inverse();
    cov_inv.setIdentity();

    MatrixXd r_A = tmp_A.transpose() * cov_inv * tmp_A;
    VectorXd r_b = tmp_A.transpose() * cov_inv * tmp_b;

    A.block&lt;6, 6&gt;(i * 3, i * 3) += r_A.topLeftCorner&lt;6, 6&gt;();
    b.segment&lt;6&gt;(i * 3) += r_b.head&lt;6&gt;();

    A.bottomRightCorner&lt;4, 4&gt;() += r_A.bottomRightCorner&lt;4, 4&gt;();
    b.tail&lt;4&gt;() += r_b.tail&lt;4&gt;();

    A.block&lt;6, 4&gt;(i * 3, n_state - 4) += r_A.topRightCorner&lt;6, 4&gt;();
    A.block&lt;4, 6&gt;(n_state - 4, i * 3) += r_A.bottomLeftCorner&lt;4, 6&gt;();
&#125;
</code></pre><p>前面计算r_A和r_b就是按照上面讲的公式写代码就行，很简单，这里主要看怎么放入最终的矩阵里，先看左边的矩阵A，我画了一个图，应该挺好理解的：<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/vgs10.png" alt="gvs9"><br>这里就拿第一个得到的r_A举例，这里把矩阵大小和每块放在哪里都画出来了，其他的以此类推。下面时右边的矩阵，其实差不多，这里也展示以下：<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/gvs13.png" alt="gvs10"><br>增量矩阵得到后，ldlt求解就行，不多说了。</p>
<hr>
<h3 id="优化重力向量-g-c-0"><a href="#优化重力向量-g-c-0" class="headerlink" title="优化重力向量$g_{c_0}$"></a>优化重力向量$g_{c_0}$</h3><p>具体原理如下，由于这部分挺简单的，就直接放深蓝学院ppt的图了：<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/gc0_1.png" alt="gc0_1.png"><br>这里就是把$\hat g^{c_0}$带入上面那个的式子，优化变量g变为$w_1,w_2$,具体式子如下：<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/gc0_2.png" alt="gc0_2.png"><br>同样是用ldlt求解。</p>
<hr>
<h3 id="bool-Estimator-visualInitialAlign"><a href="#bool-Estimator-visualInitialAlign" class="headerlink" title="bool Estimator::visualInitialAlign()"></a>bool Estimator::visualInitialAlign()</h3><p>下面总的来看这个函数<br><strong>1.计算陀螺仪偏置，尺度，重力加速度和速度</strong></p>
<pre><code>bool Estimator::visualInitialAlign()
&#123;
    TicToc t_g;
    VectorXd x;
    if(!VisualIMUAlignment(all_image_frame, Bgs, g, x))
        return false;
    ...
&#125;
</code></pre><p><strong>2.遍历所有帧，将每个图像帧到l图像帧的位移和对应的imu到l图像帧的旋转存到Ps和Rs数组，并将它们都置为关键帧</strong></p>
<pre><code>bool Estimator::visualInitialAlign()
&#123;
    // 1. 计算陀螺仪偏置，尺度，重力加速度和速度
    TicToc t_g;
    VectorXd x;
    if(!VisualIMUAlignment(all_image_frame, Bgs, g, x))
        return false;
        ...
    // 2. 传递所有图像帧的位姿Ps、Rs，并将其置为关键帧    
    for (int i = 0; i &lt;= frame_count; i++)    
    &#123;        
        Matrix3d Ri = all_image_frame[Headers[i]].R;        
        Vector3d Pi = all_image_frame[Headers[i]].T;        
        Ps[i] = Pi;        
        Rs[i] = Ri;        
        all_image_frame[Headers[i]].is_key_frame = true;    
    &#125;
    ...
&#125;
</code></pre><p><strong>3.重新计算所有f_manager的特征点深度</strong></p>
<pre><code>// 3. 重新计算所有f_manager的特征点深度    
VectorXd dep = f_manager.getDepthVector();    
for (int i = 0; i &lt; dep.size(); i++)        
    dep[i] = -1;//将所有特征点的深度置为-1    
f_manager.clearDepth(dep);

//triangulat on cam pose , no tic //重新计算特征点的深度    
Vector3d TIC_TMP[NUM_OF_CAM];    
for (int i = 0; i &lt; NUM_OF_CAM; i++)        
    TIC_TMP[i].setZero();    
    //RIC中存放的是相机到IMU的旋转，在相机-IMU外参标定部分求得    
    ric[0] = RIC[0];    
    f_manager.setRic(ric);    
    // 三角化计算地图点的深度
    // Ps中存放的是各个帧相对于参考帧之间的平移，RIC[0]为相机-IMU之间的旋转    
    f_manager.triangulate(Ps, &amp;(TIC_TMP[0]), &amp;(RIC[0]));
</code></pre><p><strong>4.IMU的bias改变，重新计算滑窗内的预积分</strong></p>
<pre><code>for (int i = 0; i &lt;= WINDOW_SIZE; i++)    
&#123;        
    pre_integrations[i]-&gt;repropagate(Vector3d::Zero(), Bgs[i]);    
&#125;
</code></pre><p><strong>5.将$P_s$、$V_s$、depth尺度s缩放后更新坐标系</strong></p>
<pre><code>// 5. 将Ps、Vs、depth尺度s缩放后从l帧转变为相对于c0帧图像坐标系下
/**           
* 前面初始化中，计算出来的是相对滑动窗口中第l帧的位姿，在这里转换到IMU bo坐标系下           
* s*p_bk^​b0​​=s*p_bk^​cl​​−s*p_b0^​cl​​=(s*p_ck^​cl​​−R_bk​^cl​​*p_c^b​)−(s*p_c0^​cl​​−R_b0​^cl​​*p_c^b​)           
* TIC[0]是相机到IMU的平移量           
* Rs是IMU第k帧imu坐标系到滑动窗口中图像第l帧的旋转           
* Ps是滑动窗口中第k帧到第l帧的平移量           
* 注意：如果运行的脚本是配置文件中无外参的脚本，那么这里的TIC都是0          
*/    
// (1) 位移Ps    
double s = (x.tail&lt;1&gt;())(0);
for (int i = frame_count; i &gt;= 0; i--)
// Ps转变为第i帧imu坐标系到第0帧imu坐标系的变换
// 之前相机第l帧为参考系，转换到IMU bo为基准坐标系        
    Ps[i] = s * Ps[i] - Rs[i] * TIC[0] - (s * Ps[0] - Rs[0] * TIC[0]);    
// （2）速度Vs
int kv = -1;   
map&lt;double, ImageFrame&gt;::iterator frame_i;    
for (frame_i = all_image_frame.begin(); frame_i != all_image_frame.end(); frame_i++)    
&#123;        
    if (frame_i-&gt;second.is_key_frame)        
    &#123;            
        kv++;            
        Vs[kv] = frame_i-&gt;second.R * x.segment&lt;3&gt;(kv * 3);        
    &#125;    
&#125;    
// (3) 深度
// 更新每个地图点被观测到的帧数(used_num)和预测的深度(estimated_depth)
for (auto &amp;it_per_id : f_manager.feature)    
&#123;        
    it_per_id.used_num = it_per_id.feature_per_frame.size();        
    if (!(it_per_id.used_num &gt;= 2 &amp;&amp; it_per_id.start_frame &lt; WINDOW_SIZE - 2))            
        continue;        
    it_per_id.estimated_depth *= s;    
&#125;
</code></pre><p><strong><em>这里强调一下，这里的Ps存的是$l$坐标系下$b_0$到$b_i$的位移，Vs存的是$l$坐标系下每帧的速度。</em></strong></p>
<p><strong>6.通过将重力旋转到z轴上，得到世界坐标系与相机坐标系$l$之间的旋转矩阵rot_diff</strong></p>
<pre><code>/**
 * 通过将g旋转至z轴方向，
 * 这样就可以计算相机系到世界坐标系的旋转矩阵，这里求得的是rot_diff,这样就可以将所有变量调整至世界系中。
*/    
// Rs是IMU第k帧到滑动窗口中图像第l帧的旋转
// R0将参考坐标系旋转到z轴垂直向上    
Matrix3d R0 = Utility::g2R(g);
// R0将参考系的y轴旋转到第0帧的IMU正前方，这个时候x轴也确定了向右。     
double yaw = Utility::R2ypr(R0 * Rs[0]).x();    
//  相机系到世界坐标系的旋转矩阵 R0    
R0 = Utility::ypr2R(Eigen::Vector3d&#123;-yaw, 0, 0&#125;) * R0; // 只考虑偏航角的影响    
g = R0 * g;      
Matrix3d rot_diff = R0;
</code></pre><p>rot_diff就是$R_{wl}$,感觉在vins里l参考帧的坐标系其实就是初始相机坐标系$c_0$</p>
<p><strong>7.所有变量从参考坐标系$l$旋转到世界坐标系w</strong></p>
<pre><code>for (int i = 0; i &lt;= frame_count; i++)    
&#123;        
    Ps[i] = rot_diff * Ps[i];        
    Rs[i] = rot_diff * Rs[i];        
    Vs[i] = rot_diff * Vs[i];    
&#125;    
//ROS_DEBUG_STREAM(&quot;g0     &quot; &lt;&lt; g.transpose());    
//ROS_DEBUG_STREAM(&quot;my R0  &quot; &lt;&lt; Utility::R2ypr(Rs[0]).transpose());
return true;
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/09/11/VINSMONO-NOTE-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/09/11/VINSMONO-NOTE-3/" itemprop="url">VINS-MONO源码阅读:初始化1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2023-09-11T10:54:17+08:00">
                2023-09-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="VINSMONO学习笔记-3"><a href="#VINSMONO学习笔记-3" class="headerlink" title="VINSMONO学习笔记(3)"></a>VINSMONO学习笔记(3)</h1><p>这篇博客开始对processImage()函数开始学习，这个函数的内容太多，这篇主要对其中初始化的部分进行学习。</p>
<hr>
<h2 id="源码阅读"><a href="#源码阅读" class="headerlink" title="源码阅读"></a>源码阅读</h2><h3 id="void-Estimator-processImage"><a href="#void-Estimator-processImage" class="headerlink" title="void Estimator::processImage()"></a>void Estimator::processImage()</h3><p>1.首先，要进行关键帧判断，这里主要是对成员变量<code>f_manager</code>进行操作，具体讲解就放在下面了；</p>
<pre><code>if (f_manager.addFeatureCheckParallax(frame_count, image, td))
    marginalization_flag = MARGIN_OLD;
else
    marginalization_flag = MARGIN_SECOND_NEW;
</code></pre><p>当滑动窗口中倒数第二图像帧为关键帧，则 marg 最老的帧，以及上面的路标点。<br>当滑动窗口中倒数第二图像帧不是关键帧，则丢弃这一帧上的视觉测量信息，IMU 预积分传给下一帧。</p>
<p>2.进行一些数据的存储和更新；</p>
<pre><code>ImageFrame imageframe(image, header.stamp.toSec());
imageframe.pre_integration = tmp_pre_integration;
all_image_frame.insert(make_pair(header.stamp.toSec(), imageframe));
tmp_pre_integration = new IntegrationBase&#123;acc_0, gyr_0, Bas[frame_count], Bgs[frame_count]&#125;;
</code></pre><p>3.如果ESTIMATE_EXTRINSIC == 2，对外参数旋转$q_{bc}$标定，写在下面了；</p>
<p>4.a</p>
<hr>
<h3 id="bool-FeatureManager-addFeatureCheckParallax"><a href="#bool-FeatureManager-addFeatureCheckParallax" class="headerlink" title="bool FeatureManager::addFeatureCheckParallax()"></a>bool FeatureManager::addFeatureCheckParallax()</h3><p>由于addFeatureCheckParallax()是FeatureManager的成员函数，且FeatureManager类的<code>f_manager</code>是<code>estimator</code>的成员变量，是经常要使用到的，由此要先介绍一下FeatureManager类。</p>
<p>这边其实只需了解成员变量<code>list&lt;FeaturePerId&gt; feature</code>该成员变量是一个list容器，容器里面的每个元素都是一个特征点id对应的所有特征点，存储在FeaturePerId类型的数据中。FeaturePerId类中有个成员变量<code>vector&lt;FeaturePerFrame&gt; feature_per_frame</code>，该id对应的所有特征点都存储在这个vector容器中，每个特征点都是FeaturePerFrame类数据，该类数据的成员变量包括特征点的归一化平面坐标，像素坐标，速度，时间戳。</p>
<p>回到addFeatureCheckParallax()函数，首先遍历传来的图像帧的所有特征点，把当前特征点封装成一个FeaturePerFrame对象并获取当前帧的feature_id，在滑窗的所有特征点中，看看能不能找到当前这个特征点：</p>
<pre><code> auto it = find_if(feature.begin(), feature.end(), [feature_id](const FeaturePerId &amp;it)&#123;
                    //是find的一个谓词判断版本，它利用返回布尔值的谓词判断pred，检查迭代器区间[first, last)上的每一个元素，
                    如果迭代器iter满足pred(*iter) == true，表示找到元素并返回迭代器值iter；
                    未找到元素，则返回last。
                    return it.feature_id == feature_id;
                &#125;);
</code></pre><p>如果这个特征点是一个新的特征(在特征点库里没有找到),那么就把它加入到滑窗的特征点库里；如果这个特征在滑窗中已经被观测到过，那么就补充上这个特征点在当前帧的数据，并且把共视点统计数+1。</p>
<p>如果窗口内只有一帧或者跟踪到的特征点小于20个，则当前帧为关键帧，返回true。</p>
<p>若不是，则遍历<code>feature</code>，如果遍历的当前id的特征点在当前帧-2以前出现过而且至少在当前帧-1还在，那么他就是平行特征点，计算该特征点在倒数第二帧和倒数第三帧之间的视差，遍历完后，求得视差总和以及平行特征点数，进行判断：</p>
<ol>
<li>平行特征点数为0,返回true。</li>
<li>平均视差&gt;=MIN_PARALLAX（值为10.0/460.0），则返回true,平均视差&lt; MIN_PARALLAX（值为10.0/460.0），则返回false。</li>
</ol>
<hr>
<h3 id="外参数旋转-q-bc-标定"><a href="#外参数旋转-q-bc-标定" class="headerlink" title="外参数旋转$q_{bc}$标定"></a>外参数旋转$q_{bc}$标定</h3><p>外参数旋转$q_{bc}$标定步骤是从下面这段代码加入的</p>
<pre><code>if(ESTIMATE_EXTRINSIC == 2)
&#123;
    ROS_INFO(&quot;calibrating extrinsic param, rotation movement is needed&quot;);
    if (frame_count != 0)
    &#123;
        vector&lt;pair&lt;Vector3d, Vector3d&gt;&gt; corres = f_manager.getCorresponding(frame_count - 1, frame_count);
        Matrix3d calib_ric;
        if (initial_ex_rotation.CalibrationExRotation(corres, pre_integrations[frame_count]-&gt;delta_q, calib_ric))
        &#123;
            ROS_WARN(&quot;initial extrinsic rotation calib success&quot;);
            ROS_WARN_STREAM(&quot;initial extrinsic rotation: &quot; &lt;&lt; endl &lt;&lt; calib_ric);
            ric[0] = calib_ric;
            RIC[0] = calib_ric;
            ESTIMATE_EXTRINSIC = 1;
        &#125;
    &#125;
&#125;
</code></pre><p>ESTIMATE_EXTRINSIC == 2表示没有$q_{bc}$标定数据，要在程序中自行标定。然后调用了函数getCorresponding(frame_count - 1, frame_count),该函数的作用是遍历f_manager中的feature容器，对于一个id的特征点，若其在最新的两帧都出现了，则将该id的特征点在两帧中的归一化坐标存为一个数据对，将所有数据对push进vector容器中。<br>上面这步将最新两帧的特征点匹配好了，然后调用函数CalibrationExRotation()进行标定，下面先放原理图，然后把原理代码结合起来讲。<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/qbc标定1.png" alt="qbc标定1"><br>这里其实就是利用了旋转约束，<strong>先将$c_{k+1}$旋转到$b_{k+1}$坐标系,再把$b_{k+1}$旋转到$b_{k}$坐标系</strong>和<strong>先将$c_{k+1}$旋转到$c_{k}$坐标系,再把$c_{k}$旋转到$b_{k}$坐标系</strong>，最终得到都是$c_{k+1}$到$b_{k}$坐标系下的旋转，是相等的，虽然这里的imu旋转没有算$b_g$偏置，但其实等式两边本来就可以把$b_g$抵消的，所以不影响。</p>
<p><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/qbc标定2.png" alt="qbc标定2"></p>
<p>接下来,看CalibrationExRotation()的具体过程。</p>
<pre><code>Rc.push_back(solveRelativeR(corres));
Rimu.push_back(delta_q_imu.toRotationMatrix());
Rc_g.push_back(ric.inverse() * delta_q_imu * ric);
Eigen::MatrixXd A(frame_count * 4, 4);
A.setZero();
</code></pre><p>首先，调用函数solveRelativeR(corres)计算两帧之间图像的旋转矩阵存到成员变量<code>Rc</code>中，这里就是用了对极约束，具体可以参考之前写的<a target="_blank" rel="noopener" href="https://someone-ob.github.io/2023/04/11/ORBSLAM3-NOTE-9/">TwoViewReconstruction.cc</a>。将两帧之间imu的旋转存到成员变量<code>Rimu</code>中，Rc_g中存储的是$q_{bc}^{ - 1} \otimes {q_{ {b_k}{b_{k + 1} } } } \otimes {q_{bc} }$。创建矩阵，它马上用来存储$Q_N$。</p>
<p>接下来按帧遍历，得到$\omega _{k + 1}^k{\rm{\cdot}}Q_{k+1}^{k}$,存到矩阵A里，具体看代码：</p>
<pre><code>Quaterniond r1(Rc[i]);
Quaterniond r2(Rc_g[i]);
double angular_distance = 180 / M_PI * r1.angularDistance(r2);
double huber = angular_distance &gt; 5.0 ? 5.0 / angular_distance : 1.0;
</code></pre><p>这段是求$\omega _{k + 1}^k$,其中180 / M_PI * r1.angularDistance(r2)用数学语言表达就是${q_{ { { \rm{c}}_k}{c_{k + 1} } } } \otimes q_{bc}^{ - 1} \otimes {q_{ {b_k}{b_{k + 1} } } } \otimes {q_{bc}}$,这个就是角度误差$r$，因为这个$q_{bc}$一开始设定的是单位阵，然后每次计算完后更新值，因此前面这个式子肯定不会等于零，故这个就是角度误差，然后再以5为阈值，就能得到$\omega _{k + 1}^k$了。</p>
<p>然后，计算四元数的左右乘矩阵，相减得到$Q_{k+1}^{k}$,具体代码如下：</p>
<pre><code>Matrix4d L, R;

double w = Quaterniond(Rc[i]).w();
Vector3d q = Quaterniond(Rc[i]).vec();
L.block&lt;3, 3&gt;(0, 0) = w * Matrix3d::Identity() + Utility::skewSymmetric(q);
L.block&lt;3, 1&gt;(0, 3) = q;
L.block&lt;1, 3&gt;(3, 0) = -q.transpose();
L(3, 3) = w;

Quaterniond R_ij(Rimu[i]);
w = R_ij.w();
q = R_ij.vec();
R.block&lt;3, 3&gt;(0, 0) = w * Matrix3d::Identity() - Utility::skewSymmetric(q);
R.block&lt;3, 1&gt;(0, 3) = q;
R.block&lt;1, 3&gt;(3, 0) = -q.transpose();
R(3, 3) = w;

A.block&lt;4, 4&gt;((i - 1) * 4, 0) = huber * (L - R);
</code></pre><p>这里没太懂，我搜到的资料和博客里，左右乘矩阵都不长这样。。。先放在这里，以后懂了再来更新。</p>
<p>最后，svd分解求解，迭代计算了&gt;=WINDOW_SIZE次并且ric_cov(1) &gt; 0.25，则认为外参标定成功，否则返回false，等再有新帧进来时再调用标定函数再次标定。</p>
<pre><code>JacobiSVD&lt;MatrixXd&gt; svd(A, ComputeFullU | ComputeFullV);
Matrix&lt;double, 4, 1&gt; x = svd.matrixV().col(3);
Quaterniond estimated_R(x);
ric = estimated_R.toRotationMatrix().inverse();
//cout &lt;&lt; svd.singularValues().transpose() &lt;&lt; endl;
//cout &lt;&lt; ric &lt;&lt; endl;
Vector3d ric_cov;
ric_cov = svd.singularValues().tail&lt;3&gt;();
if (frame_count &gt;= WINDOW_SIZE &amp;&amp; ric_cov(1) &gt; 0.25)
&#123;
    calib_ric_result = ric;
    return true;
&#125;
else
    return false;
</code></pre><hr>
<h3 id="Sfm优化滑窗内位姿"><a href="#Sfm优化滑窗内位姿" class="headerlink" title="Sfm优化滑窗内位姿"></a>Sfm优化滑窗内位姿</h3><p><strong>1.确保IMU有足够的激励</strong></p>
<ul>
<li>这里涉及两个循环，分别求出滑动窗口的平均线加速度和滑窗内的线加速度的标准差<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">map&lt;double, ImageFrame&gt;::iterator frame_it;</span><br><span class="line">Vector3d sum_g;</span><br><span class="line">//求出滑动窗口的平均线加速度</span><br><span class="line">for (frame_it = all_image_frame.begin(), frame_it++; frame_it != all_image_frame.end(); frame_it++)</span><br><span class="line">&#123;</span><br><span class="line">    double dt = frame_it-&gt;second.pre_integration-&gt;sum_dt;</span><br><span class="line">    Vector3d tmp_g = frame_it-&gt;second.pre_integration-&gt;delta_v / dt;</span><br><span class="line">    sum_g += tmp_g;</span><br><span class="line">&#125;</span><br><span class="line">Vector3d aver_g;</span><br><span class="line">aver_g = sum_g * 1.0 / ((int)all_image_frame.size() - 1);</span><br><span class="line"></span><br><span class="line">//求出滑窗内的线加速度的标准差</span><br><span class="line">double var = 0;</span><br><span class="line">for (frame_it = all_image_frame.begin(), frame_it++; frame_it != all_image_frame.end(); frame_it++)</span><br><span class="line">&#123;</span><br><span class="line">    double dt = frame_it-&gt;second.pre_integration-&gt;sum_dt;</span><br><span class="line">    Vector3d tmp_g = frame_it-&gt;second.pre_integration-&gt;delta_v / dt;</span><br><span class="line">    var += (tmp_g - aver_g).transpose() * (tmp_g - aver_g);</span><br><span class="line">    //cout &lt;&lt; &quot;frame g &quot; &lt;&lt; tmp_g.transpose() &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line">var = sqrt(var / ((int)all_image_frame.size() - 1));</span><br></pre></td></tr></table></figure>
最后根据标准差大小决定是否返回false。源代码中返回false是被注释掉的，可以删掉注释符号启动该功能。</li>
</ul>
<p><strong>2.将f_manager中的所有feature在所有帧的归一化坐标保存到容器sfm_f中</strong><br>这里首先是定义了一些变量用来存取数据，这里出现了一个新的类型SFMFeature</p>
<pre><code>Quaterniond Q[frame_count + 1];
Vector3d T[frame_count + 1];
map&lt;int, Vector3d&gt; sfm_tracked_points;
vector&lt;SFMFeature&gt; sfm_f;
</code></pre><p><em>这里Q，T的数组大小是frame_count + 1是因为滑窗的容量是10，再加上当前最新帧，所以需要储存11帧的值。</em> 下面看一下SFMFeature类包含哪些成员变量</p>
<pre><code>struct SFMFeature
&#123;
    bool state;//状态（是否被三角化）
    int id;
    vector&lt;pair&lt;int,Vector2d&gt;&gt; observation;//所有观测到该特征点的 图像帧ID 和 特征点在这个图像帧的归一化坐标
    double position[3];//在帧l下的空间坐标
    double depth;//深度
&#125;;   
</code></pre><p>下面就是遍历f_manager的feature变量，将需要的信息为SFMFeature类型的变量赋值，然后存到vector<SFMFeature>容器中，实现对sfm_f的赋值。</p>
<p><strong>3.在滑窗(0-9)中找到第一个满足要求的帧(第l帧)，它与最新一帧(frame_count=10)有足够的共视点和平行度，并求出这两帧之间的相对位置变化关系</strong><br>这里主要调用了函数relativePose()</p>
<pre><code>Matrix3d relative_R;
Vector3d relative_T;
int l;
if (!relativePose(relative_R, relative_T, l))
&#123;
    ROS_INFO(&quot;Not enough features or parallax; Move device around&quot;);
    return false;
&#125;
</code></pre><p>下面细看一下relativePose()函数：</p>
<pre><code>bool Estimator::relativePose(Matrix3d &amp;relative_R, Vector3d &amp;relative_T, int &amp;l)
&#123;
    // find previous frame which contians enough correspondance and parallex with newest frame
    for (int i = 0; i &lt; WINDOW_SIZE; i++)
    &#123;
        vector&lt;pair&lt;Vector3d, Vector3d&gt;&gt; corres;
        corres = f_manager.getCorresponding(i, WINDOW_SIZE);
        if (corres.size() &gt; 20)
        &#123;
            double sum_parallax = 0;
            double average_parallax;
            for (int j = 0; j &lt; int(corres.size()); j++)
            &#123;
                Vector2d pts_0(corres[j].first(0), corres[j].first(1));
                Vector2d pts_1(corres[j].second(0), corres[j].second(1));
                double parallax = (pts_0 - pts_1).norm();
                sum_parallax = sum_parallax + parallax;

            &#125;
            average_parallax = 1.0 * sum_parallax / int(corres.size());
            if(average_parallax * 460 &gt; 30 &amp;&amp; m_estimator.solveRelativeRT(corres, relative_R, relative_T))
            &#123;
                l = i;
                ROS_DEBUG(&quot;average_parallax %f choose l %d and newest frame to triangulate the whole structure&quot;, average_parallax * 460, l);
                return true;
            &#125;
        &#125;
    &#125;
    return false;
&#125;
</code></pre><p>这里是遍历滑动窗口帧数，调用函数getCorresponding(i, WINDOW_SIZE)，这个函数是遍历所有id对应的特征点，加入该特征点在i帧前就出现了，且到WINDOW_SIZE帧还在，就将该数据点在i帧和WINDOW_SIZE帧的归一化平面坐标组成数据对。得到所有数据对后，计算平均视差，进行判断：</p>
<pre><code>if(average_parallax * 460 &gt; 30 &amp;&amp; m_estimator.solveRelativeRT(corres, relative_R, relative_T))
</code></pre><p>这里又调用了函数solveRelativeRT(corres, relative_R, relative_T)，这个函数计算了i帧和WINDOW_SIZE帧之间的位姿变换(也是根据对极约束)，并根据匹配点的内点数量是否大于12，确定返回值。<br>若判断通过，则将i的值赋给l，作为参考帧的索引。</p>
<p><strong>4.调用construct()函数，对窗口中每个图像帧求解sfm问题</strong><br><strong>(1) 把第l帧作为参考坐标系，获得最新一帧在参考坐标系下的位姿</strong></p>
<pre><code>feature_num = sfm_f.size(); 
q[l].w() = 1; //参考帧的四元数，平移为1和0
q[l].x() = 0;
q[l].y() = 0;
q[l].z() = 0;
T[l].setZero(); //1、这里把第l帧看作参考坐标系，根据当前帧到第l帧的relative_R，relative_T，得到当前帧在参考坐标系下的位姿，之后的pose[i]表示第l帧到第i帧的变换矩阵[R|T]
q[frame_num - 1] = q[l] * Quaterniond(relative_R); //frame_num-1表示当前帧* relative c0_-&gt;ck
T[frame_num - 1] = relative_T;
</code></pre><p>这里的q和T数组存储着每帧到参考帧的旋转和平移。</p>
<p><strong>(2)构造容器，存储滑窗内第l帧相对于其它帧和最新一帧的位姿</strong></p>
<pre><code>Matrix3d c_Rotation[frame_num]; 
Vector3d c_Translation[frame_num];
Quaterniond c_Quat[frame_num];
double c_rotation[frame_num][4];
double c_translation[frame_num][3]; 
Eigen::Matrix&lt;double, 3, 4&gt; Pose[frame_num];
</code></pre><p>定义了如上所示容器，这些用来做三角化并当作非线性优化的优化量。由于目前l帧和最新帧位姿已知，则先给它们赋值：</p>
<pre><code>c_Quat[l] = q[l].inverse();
c_Rotation[l] = c_Quat[l].toRotationMatrix();
c_Translation[l] = -1 * (c_Rotation[l] * T[l]);
Pose[l].block&lt;3, 3&gt;(0, 0) = c_Rotation[l];
Pose[l].block&lt;3, 1&gt;(0, 3) = c_Translation[l];

c_Quat[frame_num - 1] = q[frame_num - 1].inverse();
c_Rotation[frame_num - 1] = c_Quat[frame_num - 1].toRotationMatrix();
c_Translation[frame_num - 1] = -1 * (c_Rotation[frame_num - 1] * T[frame_num - 1]);
Pose[frame_num - 1].block&lt;3, 3&gt;(0, 0) = c_Rotation[frame_num - 1];
Pose[frame_num - 1].block&lt;3, 1&gt;(0, 3) = c_Translation[frame_num - 1];
</code></pre><p><strong>(3)使用pnp恢复滑动窗口内其他帧的位姿并通过三角化恢复它们与最新帧共识点的3维坐标</strong></p>
<pre><code>for (int i = l; i &lt; frame_num - 1 ; i++)
&#123;
    // solve pnp
    if (i &gt; l)
    &#123;
        Matrix3d R_initial = c_Rotation[i - 1];
        Vector3d P_initial = c_Translation[i - 1];
        if(!solveFrameByPnP(R_initial, P_initial, i, sfm_f))
            return false;
        c_Rotation[i] = R_initial;
        c_Translation[i] = P_initial;
        c_Quat[i] = c_Rotation[i];
        Pose[i].block&lt;3, 3&gt;(0, 0) = c_Rotation[i];
        Pose[i].block&lt;3, 1&gt;(0, 3) = c_Translation[i];
    &#125;

    // triangulate point based on the solve pnp result
    triangulateTwoFrames(i, Pose[i], frame_num - 1, Pose[frame_num - 1], sfm_f);
&#125;
</code></pre><p>这里主要涉及两个函数<code>solveFrameByPnP()</code>和<code>triangulateTwoFrames()</code>。先看<code>triangulateTwoFrames()</code>的部分代码：</p>
<pre><code>for (int j = 0; j &lt; feature_num; j++)
&#123;
    if (sfm_f[j].state == true)
        continue;
    bool has_0 = false, has_1 = false;
    Vector2d point0;
    Vector2d point1;
    for (int k = 0; k &lt; (int)sfm_f[j].observation.size(); k++)
    &#123;
        if (sfm_f[j].observation[k].first == frame0)
        &#123;
            point0 = sfm_f[j].observation[k].second;
            has_0 = true;
        &#125;
        if (sfm_f[j].observation[k].first == frame1)
        &#123;
            point1 = sfm_f[j].observation[k].second;
            has_1 = true;
        &#125;
    &#125;
    ...
    ...
&#125;
</code></pre><p>这里就是遍历所有id的特征点，再对每个id的特征点进行遍历，假如该id的特征点在目前刚恢复位姿的那帧和最新帧都出现则将该特征点恢复3为坐标，上面省略的代码就是恢复步骤，具体原理参考之前写的<a target="_blank" rel="noopener" href="https://someone-ob.github.io/2023/04/11/ORBSLAM3-NOTE-9/">TwoViewReconstruction.cc</a>三角化部分，就是把最后的那个公式转成代码就行。</p>
<p>接下来看solveFrameByPnP()的具体实现，包括以下几步：</p>
<ul>
<li>把滑窗的所有特征点中，那些没有3D坐标的点pass掉,并且只把被当前帧观测到的特征点的归一化坐标及其三维坐标。  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">   for (int j = 0; j &lt; feature_num; j++)</span><br><span class="line">&#123;</span><br><span class="line">	if (sfm_f[j].state != true)</span><br><span class="line">		continue;</span><br><span class="line">	Vector2d point2d;</span><br><span class="line">	for (int k = 0; k &lt; (int)sfm_f[j].observation.size(); k++)</span><br><span class="line">	&#123;</span><br><span class="line">		if (sfm_f[j].observation[k].first == i)</span><br><span class="line">		&#123;</span><br><span class="line">			Vector2d img_pts = sfm_f[j].observation[k].second;</span><br><span class="line">			cv::Point2f pts_2(img_pts(0), img_pts(1));</span><br><span class="line">			pts_2_vector.push_back(pts_2);</span><br><span class="line">			cv::Point3f pts_3(sfm_f[j].position[0], sfm_f[j].position[1], sfm_f[j].position[2]);</span><br><span class="line">			pts_3_vector.push_back(pts_3);</span><br><span class="line">			break;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>如果这些有3D坐标的特征点，数量少于15，那么整个初始化全部失败。  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">   if (int(pts_2_vector.size()) &lt; 15)</span><br><span class="line">&#123;</span><br><span class="line">	printf(&quot;unstable features tracking, please slowly move you device!\n&quot;);</span><br><span class="line">	if (int(pts_2_vector.size()) &lt; 10)</span><br><span class="line">		return false;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>套用openCV的公式，进行PnP求解。  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">   cv::Mat r, rvec, t, D, tmp_r;</span><br><span class="line">cv::eigen2cv(R_initial, tmp_r);</span><br><span class="line">cv::Rodrigues(tmp_r, rvec);</span><br><span class="line">cv::eigen2cv(P_initial, t);</span><br><span class="line">cv::Mat K = (cv::Mat_&lt;double&gt;(3, 3) &lt;&lt; 1, 0, 0, 0, 1, 0, 0, 0, 1);</span><br><span class="line">bool pnp_succ;</span><br><span class="line">pnp_succ = cv::solvePnP(pts_3_vector, pts_2_vector, K, D, rvec, t, 1);</span><br><span class="line">if(!pnp_succ)</span><br><span class="line">&#123;</span><br><span class="line">	return false;</span><br><span class="line">&#125;</span><br><span class="line">cv::Rodrigues(rvec, r);</span><br><span class="line">//cout &lt;&lt; &quot;r &quot; &lt;&lt; endl &lt;&lt; r &lt;&lt; endl;</span><br><span class="line">MatrixXd R_pnp;</span><br><span class="line">cv::cv2eigen(r, R_pnp);</span><br><span class="line">MatrixXd T_pnp;</span><br><span class="line">cv::cv2eigen(t, T_pnp);</span><br><span class="line">R_initial = R_pnp;</span><br><span class="line">P_initial = T_pnp;</span><br><span class="line">return true;</span><br></pre></td></tr></table></figure>
  这里注意两点，R_initial和P_initial是作为形参传过来的，是上一帧的位姿，目的是作为迭代的初始值，加快迭代。还有这里的内参矩阵K，这里是单位矩阵，因为这个重投影误差里的像素坐标用的其实是<code>feature_tracker_node</code>节点传来的归一化坐标，已经乘过内参的逆了。</li>
</ul>
<p><strong>(4)将第l+1帧到滑窗的最后的每一帧再与第l帧进行三角化补充3D坐标</strong></p>
<pre><code>for (int i = l + 1; i &lt; frame_num - 1; i++)
    triangulateTwoFrames(l, Pose[l], i, Pose[i], sfm_f);
</code></pre><p><strong>(5)用pnp求第l帧之前的每一帧的位姿，并求它们与第l帧的共视点的3D坐标</strong><br>这步和第(3)步基本没区别，就不放代码了。</p>
<p><strong>(6)将剩余未三角化特征点恢复三维坐标</strong></p>
<pre><code>for (int j = 0; j &lt; feature_num; j++)
&#123;
    if (sfm_f[j].state == true)
        continue;
    if ((int)sfm_f[j].observation.size() &gt;= 2)
    &#123;
        Vector2d point0, point1;
        int frame_0 = sfm_f[j].observation[0].first;
        point0 = sfm_f[j].observation[0].second;
        int frame_1 = sfm_f[j].observation.back().first;
        point1 = sfm_f[j].observation.back().second;
        Vector3d point_3d;
        triangulatePoint(Pose[frame_0], Pose[frame_1], point0, point1, point_3d);
        sfm_f[j].state = true;
        sfm_f[j].position[0] = point_3d(0);
        sfm_f[j].position[1] = point_3d(1);
        sfm_f[j].position[2] = point_3d(2);
        //cout &lt;&lt; &quot;trangulated : &quot; &lt;&lt; frame_0 &lt;&lt; &quot; &quot; &lt;&lt; frame_1 &lt;&lt; &quot;  3d point : &quot;  &lt;&lt; j &lt;&lt; &quot;  &quot; &lt;&lt; point_3d.transpose() &lt;&lt; endl;
    &#125;        
&#125;
</code></pre><p>这里也是用的三角化的方法，这里用的是未三角化特征点的起始帧和末尾帧。</p>
<p><strong>(7)采用ceres进行全局BA</strong></p>
<ul>
<li>创建problem对象，加入待优化量，即全局位姿，固定第l帧位姿不优化。  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">   ceres::Problem problem;</span><br><span class="line">ceres::LocalParameterization* local_parameterization = new ceres::QuaternionParameterization();</span><br><span class="line">//cout &lt;&lt; &quot; begin full BA &quot; &lt;&lt; endl;</span><br><span class="line">for (int i = 0; i &lt; frame_num; i++)</span><br><span class="line">&#123;</span><br><span class="line">	//double array for ceres</span><br><span class="line">	c_translation[i][0] = c_Translation[i].x();</span><br><span class="line">	c_translation[i][1] = c_Translation[i].y();</span><br><span class="line">	c_translation[i][2] = c_Translation[i].z();</span><br><span class="line">	c_rotation[i][0] = c_Quat[i].w();</span><br><span class="line">	c_rotation[i][1] = c_Quat[i].x();</span><br><span class="line">	c_rotation[i][2] = c_Quat[i].y();</span><br><span class="line">	c_rotation[i][3] = c_Quat[i].z();</span><br><span class="line">	problem.AddParameterBlock(c_rotation[i], 4, local_parameterization);</span><br><span class="line">	problem.AddParameterBlock(c_translation[i], 3);</span><br><span class="line">	if (i == l)</span><br><span class="line">	&#123;</span><br><span class="line">		problem.SetParameterBlockConstant(c_rotation[i]);</span><br><span class="line">	&#125;</span><br><span class="line">	if (i == l || i == frame_num - 1)</span><br><span class="line">	&#123;</span><br><span class="line">		problem.SetParameterBlockConstant(c_translation[i]);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>加入残差块  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">   for (int i = 0; i &lt; feature_num; i++)</span><br><span class="line">&#123;</span><br><span class="line">	if (sfm_f[i].state != true)</span><br><span class="line">		continue;</span><br><span class="line">	for (int j = 0; j &lt; int(sfm_f[i].observation.size()); j++)</span><br><span class="line">	&#123;</span><br><span class="line">		int l = sfm_f[i].observation[j].first;</span><br><span class="line">		ceres::CostFunction* cost_function = ReprojectionError3D::Create(sfm_f[i].observation[j].second.x(),</span><br><span class="line">											sfm_f[i].observation[j].second.y());</span><br><span class="line"></span><br><span class="line">   		problem.AddResidualBlock(cost_function, NULL, c_rotation[l], c_translation[l], </span><br><span class="line">   								sfm_f[i].position);	 </span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
  problem.AddResidualBlock()中的参数分别是代价函数，损失函数(这里为空)，待优化变量。这里详细看一下代价函数：   <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceres::CostFunction* cost_function = ReprojectionError3D::Create(sfm_f[i].observation[j].second.x(),</span><br><span class="line">									sfm_f[i].observation[j].second.y());</span><br></pre></td></tr></table></figure>
  这里调用了ReprojectionError3D::Create(),下面看一下ReprojectionError3D类：  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">   struct ReprojectionError3D</span><br><span class="line">   &#123;</span><br><span class="line">ReprojectionError3D(double observed_u, double observed_v)</span><br><span class="line">	:observed_u(observed_u), observed_v(observed_v)</span><br><span class="line">	&#123;&#125;</span><br><span class="line"></span><br><span class="line">template &lt;typename T&gt;</span><br><span class="line">bool operator()(const T* const camera_R, const T* const camera_T, const T* point, T* residuals) const</span><br><span class="line">&#123;</span><br><span class="line">	T p[3];</span><br><span class="line">	ceres::QuaternionRotatePoint(camera_R, point, p);</span><br><span class="line">	p[0] += camera_T[0]; p[1] += camera_T[1]; p[2] += camera_T[2];</span><br><span class="line">	T xp = p[0] / p[2];</span><br><span class="line">   	T yp = p[1] / p[2];</span><br><span class="line">   	residuals[0] = xp - T(observed_u);</span><br><span class="line">   	residuals[1] = yp - T(observed_v);</span><br><span class="line">   	return true;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static ceres::CostFunction* Create(const double observed_x,</span><br><span class="line">                                   const double observed_y) </span><br><span class="line">&#123;</span><br><span class="line">  return (new ceres::AutoDiffCostFunction&lt;</span><br><span class="line">          ReprojectionError3D, 2, 4, 3, 3&gt;(</span><br><span class="line">          	new ReprojectionError3D(observed_x,observed_y)));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">double observed_u;</span><br><span class="line">double observed_v;</span><br><span class="line">   &#125;;</span><br></pre></td></tr></table></figure>
  这里的bool operator()实现重载()运算，这里的功能是计算重投影误差，能够在Ceres Solver中被调用，并计算出当前数据点的残差。Ceres Solver会在优化过程中自动调用这个函数对象来计算残差，并最小化残差以拟合模型。<br>  Create()函数的返回值是ceres::AutoDiffCostFunction对象，这里的<ReprojectionError3D, 2, 4, 3, 3>分别是误差类型，输出维度，输入维度。输出即残差项(像素坐标)，故维度是2；输入就是上面相机旋转，平移，特征点3维坐标，与bool operator()所需参数一致，故维度维4，3，3。最后在括号里将自定义的误差函数传递给它。<br>  &nbsp;</li>
<li>shur消元求解  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">   ceres::Solver::Options options;</span><br><span class="line">options.linear_solver_type = ceres::DENSE_SCHUR;</span><br><span class="line">//options.minimizer_progress_to_stdout = true;</span><br><span class="line">options.max_solver_time_in_seconds = 0.2;</span><br><span class="line">ceres::Solver::Summary summary;</span><br><span class="line">ceres::Solve(options, &amp;problem, &amp;summary);</span><br><span class="line">//std::cout &lt;&lt; summary.BriefReport() &lt;&lt; &quot;\n&quot;;</span><br><span class="line">if (summary.termination_type == ceres::CONVERGENCE || summary.final_cost &lt; 5e-03)</span><br><span class="line">&#123;</span><br><span class="line">	//cout &lt;&lt; &quot;vision only BA converge&quot; &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line">else</span><br><span class="line">&#123;</span><br><span class="line">	//cout &lt;&lt; &quot;vision only BA not converge &quot; &lt;&lt; endl;</span><br><span class="line">	return false;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
  shur消元有2大作用，一个是在最小二乘中利用H矩阵稀疏的性质进行加速求解，这个在slam14讲里讲得很细；另一个是在sliding window时求解marg掉老帧后的先验信息矩阵，这个在深蓝学院的VIO课程里讲了很长篇幅，也很好。这块是shur消元的第一个用法。<br>  &nbsp;</li>
<li>返回特征点l系下3D坐标和优化后的全局位姿  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">   for (int i = 0; i &lt; frame_num; i++)</span><br><span class="line">&#123;</span><br><span class="line">	q[i].w() = c_rotation[i][0]; </span><br><span class="line">	q[i].x() = c_rotation[i][1]; </span><br><span class="line">	q[i].y() = c_rotation[i][2]; </span><br><span class="line">	q[i].z() = c_rotation[i][3]; </span><br><span class="line">	q[i] = q[i].inverse();</span><br><span class="line">	//cout &lt;&lt; &quot;final  q&quot; &lt;&lt; &quot; i &quot; &lt;&lt; i &lt;&lt;&quot;  &quot; &lt;&lt;q[i].w() &lt;&lt; &quot;  &quot; &lt;&lt; q[i].vec().transpose() &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line">for (int i = 0; i &lt; frame_num; i++)</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">	T[i] = -1 * (q[i] * Vector3d(c_translation[i][0], c_translation[i][1], c_translation[i][2]));</span><br><span class="line">	//cout &lt;&lt; &quot;final  t&quot; &lt;&lt; &quot; i &quot; &lt;&lt; i &lt;&lt;&quot;  &quot; &lt;&lt; T[i](0) &lt;&lt;&quot;  &quot;&lt;&lt; T[i](1) &lt;&lt;&quot;  &quot;&lt;&lt; T[i](2) &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line">for (int i = 0; i &lt; (int)sfm_f.size(); i++)</span><br><span class="line">&#123;</span><br><span class="line">	if(sfm_f[i].state)</span><br><span class="line">		sfm_tracked_points[sfm_f[i].id] = Vector3d(sfm_f[i].position[0], sfm_f[i].position[1], sfm_f[i].position[2]);</span><br><span class="line">&#125;</span><br><span class="line">return true;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>(8)给滑窗外的图像帧提供初始的RT估计，然后solvePnP进行优化</strong></p>
<ul>
<li>对于滑窗内的帧，把它们设为关键帧，并<strong>获得它们对应的IMU坐标系到l系的旋转,以及每个图像帧到l帧的平移，即$R_{lb_k},t_{lc_k}$，通过frame_it存到all_image_frame中</strong>。  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">if((frame_it-&gt;first) == Headers[i].stamp.toSec())</span><br><span class="line">&#123;</span><br><span class="line">    frame_it-&gt;second.is_key_frame = true;</span><br><span class="line">    frame_it-&gt;second.R = Q[i].toRotationMatrix() * RIC[0].transpose();</span><br><span class="line">    frame_it-&gt;second.T = T[i];</span><br><span class="line">    i++;</span><br><span class="line">    continue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>如果当前帧的时间戳大于滑窗内第i帧的时间戳，那么i++  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">//TODO delete this judgement</span><br><span class="line">if((frame_it-&gt;first) &gt; Headers[i].stamp.toSec())</span><br><span class="line">    i++; //TODO change to ++i</span><br></pre></td></tr></table></figure>
  有一说一，这个判断应该基本不会执行吧，就算按它要执行的逻辑，也应该在外面加个i&lt; WINDOW_SIZE+1，这里感觉没太看懂。。。<br>  &nbsp;</li>
<li>对滑窗外的所有帧，求出它们对应的IMU坐标系到l帧的旋转,以及每个图像帧到l帧的平移，即$R_{lb_k},t_{lc_k}$，通过frame_it存到all_image_frame中。  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">//注意这里的 Q和 T是图像帧的位姿，而不是求解PNP时所用的坐标系变换矩阵，两者具有对称关系</span><br><span class="line">Matrix3d R_inital = (Q[i].inverse()).toRotationMatrix();</span><br><span class="line">Vector3d P_inital = - R_inital * T[i];</span><br><span class="line">cv::eigen2cv(R_inital, tmp_r);</span><br><span class="line">cv::Rodrigues(tmp_r, rvec);//罗德里格斯公式将旋转矩阵转换成旋转向量</span><br><span class="line">cv::eigen2cv(P_inital, t);</span><br><span class="line"></span><br><span class="line">frame_it-&gt;second.is_key_frame = false;</span><br><span class="line">vector&lt;cv::Point3f&gt; pts_3_vector;//获取 pnp需要用到的存储每个特征点三维点和图像坐标的 vector</span><br><span class="line">vector&lt;cv::Point2f&gt; pts_2_vector;</span><br><span class="line">for (auto &amp;id_pts : frame_it-&gt;second.points)</span><br><span class="line">&#123;</span><br><span class="line">    int feature_id = id_pts.first;</span><br><span class="line">    for (auto &amp;i_p : id_pts.second)</span><br><span class="line">    &#123;</span><br><span class="line">        it = sfm_tracked_points.find(feature_id);</span><br><span class="line">        if(it != sfm_tracked_points.end())</span><br><span class="line">        &#123;</span><br><span class="line">            Vector3d world_pts = it-&gt;second;</span><br><span class="line">            cv::Point3f pts_3(world_pts(0), world_pts(1), world_pts(2));</span><br><span class="line">            pts_3_vector.push_back(pts_3);</span><br><span class="line">            Vector2d img_pts = i_p.second.head&lt;2&gt;();</span><br><span class="line">            cv::Point2f pts_2(img_pts(0), img_pts(1));</span><br><span class="line">            pts_2_vector.push_back(pts_2);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;//保证特征点数大于 5</span><br><span class="line">cv::Mat K = (cv::Mat_&lt;double&gt;(3, 3) &lt;&lt; 1, 0, 0, 0, 1, 0, 0, 0, 1);     </span><br><span class="line">if(pts_3_vector.size() &lt; 6)</span><br><span class="line">&#123;</span><br><span class="line">    cout &lt;&lt; &quot;pts_3_vector size &quot; &lt;&lt; pts_3_vector.size() &lt;&lt; endl;</span><br><span class="line">    ROS_DEBUG(&quot;Not enough points for solve pnp !&quot;);</span><br><span class="line">    return false;</span><br><span class="line">&#125;</span><br><span class="line">if (! cv::solvePnP(pts_3_vector, pts_2_vector, K, D, rvec, t, 1))</span><br><span class="line">&#123;</span><br><span class="line">    ROS_DEBUG(&quot;solve pnp fail!&quot;);</span><br><span class="line">    return false;</span><br><span class="line">&#125;</span><br><span class="line">cv::Rodrigues(rvec, r);</span><br><span class="line">MatrixXd R_pnp,tmp_R_pnp;</span><br><span class="line">cv::cv2eigen(r, tmp_R_pnp);</span><br><span class="line">R_pnp = tmp_R_pnp.transpose();</span><br><span class="line">MatrixXd T_pnp;</span><br><span class="line">cv::cv2eigen(t, T_pnp);</span><br><span class="line">T_pnp = R_pnp * (-T_pnp);</span><br><span class="line">frame_it-&gt;second.R = R_pnp * RIC[0].transpose();</span><br><span class="line">frame_it-&gt;second.T = T_pnp;</span><br></pre></td></tr></table></figure></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/09/09/VINSMONO-NOTE-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/09/09/VINSMONO-NOTE-2/" itemprop="url">VINS-MONO源码阅读:预积分</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2023-09-09T17:21:24+08:00">
                2023-09-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="VINSMONO学习笔记-2"><a href="#VINSMONO学习笔记-2" class="headerlink" title="VINSMONO学习笔记(2)"></a>VINSMONO学习笔记(2)</h1><p>这篇博客开始就是对功能包<code>estimator_node</code>的学习了，这个功能包实现了预积分，初始化，非线性优化，闭环检测功能，这篇重点把预积分部分的代码梳理清楚。</p>
<hr>
<h2 id="源码阅读"><a href="#源码阅读" class="headerlink" title="源码阅读"></a>源码阅读</h2><h3 id="main-函数"><a href="#main-函数" class="headerlink" title="main()函数"></a>main()函数</h3><p>main()函数中，首先调用函数readParameters(n)从yaml文件中读取相关参数，至于读了哪些参数等用到再解释。然后，estimator.setParameter()读取了每一个相机到IMU坐标系的旋转/平移外参数和非线性优化的重投影误差部分的信息矩阵。<strong><code>estimator</code>这个变量相当重要，之后基本都是对其进行操作，这里建议把他的缺省构造函数好好看一下。</strong><br>接下来，registerPub(n)发布用于RVIZ显示的Topic。然后就是定义了四个订阅方，接收对应的信息，并启动一个子线程调用函数<code>process()</code>。下面会着重讲回调函数<code>imu_callback()</code>和<code>process()</code>中涉及预积分的部分。</p>
<hr>
<h3 id="void-imu-callback"><a href="#void-imu-callback" class="headerlink" title="void imu_callback()"></a>void imu_callback()</h3><p>这个函数是只要接收到imu数据就会执行。首先，先更新最新的imu数据时间<code>last_imu_t</code>,往<code>imu_buf</code>里放IMU数据，缓存起来。con.notify_one()这句代码用处是每次执行后就会唤醒函数<code>process()</code>中的con.wait(…),执行该语句。这里的代码是要用到线程锁的，避免imu_buf存取数据冲突。</p>
<pre><code>last_imu_t = imu_msg-&gt;header.stamp.toSec();
m_buf.lock();
imu_buf.push(imu_msg);
m_buf.unlock();
con.notify_one();
last_imu_t = imu_msg-&gt;header.stamp.toSec();

&#123;
    std::lock_guard&lt;std::mutex&gt; lg(m_state);
    predict(imu_msg);
    std_msgs::Header header = imu_msg-&gt;header;
    header.frame_id = &quot;world&quot;;
    if (estimator.solver_flag == Estimator::SolverFlag::NON_LINEAR)
        pubLatestOdometry(tmp_P, tmp_Q, tmp_V, header);
&#125;
</code></pre><p>然后，执行函数predict(imu_msg)，该函数就是读取传来的imu数据，使用中值法求解PVQ，更新tmp_Q，tmp_P，tmp_V的值，然后更新acc_0和gyr_0，作为下一次中执法的初始值，具体公式如下图所示：<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/中值法.png" alt="中值法"><br>这里的数据都是世界坐标系下的。其实未初始化时，这个函数虽然在跑，但得到的结果都是无效的，因为此时该函数里的<code>estimator.g</code>，<code>tmp_Ba</code>,<code>tmp_Bg</code>一直是(0,0,0)。<br>最后，如果当前处于非线性优化阶段的话，需要把predict函数计算得到的PVQ发布到rviz里去，见utility/visualization.cpp的pubLatestOdometry()函数。</p>
<hr>
<h3 id="void-process"><a href="#void-process" class="headerlink" title="void process()"></a>void process()</h3><p>1.获得measurements数据，首先先要看一下measurements的数据类型，还是挺复杂的。</p>
<pre><code>std::vector&lt;std::pair&lt;std::vector&lt;sensor_msgs::ImuConstPtr&gt;, sensor_msgs::PointCloudConstPtr&gt;&gt; measurements;
</code></pre><p><code>measurements</code>最外层看是一个向量容器，这个容器中存储一些数据对（std::pair），然后再看单个数据对，它的第一位也是一个容器，里面存放着一段时间下的imu数据，第二位存放的是一帧图像中的特征点数据，就是<code>feature_tracker_node</code>传过来的，至于具体怎么把数据放进measurements，这些数据又是怎么选取配对的，要看函数getMeasurements()，这个函数里主要就是有几个判断：</p>
<ul>
<li><strong>if (imu_buf.empty() || feature_buf.empty())</strong>，这个说明数据取完了，说明配对完成，直接返回；</li>
<li><strong>if (!(imu_buf.back()-&gt;header.stamp.toSec() &gt; feature_buf.front()-&gt;header.stamp.toSec() + estimator.td))</strong>，这里imu_buf里面所有数据的时间戳都比img buf第一个帧时间戳要早，说明缺乏IMU数据，需要等待IMU数据；</li>
<li><strong>if (!(imu_buf.front()-&gt;header.stamp.toSec() &lt; feature_buf.front()-&gt;header.stamp.toSec() + estimator.td))</strong>，IMU第一个数据的时间要大于第一个图像特征数据的时间，说明有图像帧太老了，讲<code>feature_buf</code>中的数据pop出一个，然后直接从头判断；</li>
</ul>
<p>上面的判断结束后，从<code>feature_buf</code>中pop出一个存到img_msg中，然后遍历imu_buf，讲时间戳小于img_msg的imu数据存到容器IMUs中，然后将IMUs和img_msg作为一个数据对存到measurements中。</p>
<p>2.遍历measurements，对于每个measurement，先遍历所有imu数据，读取imu数据的加速度和角速度，然后调用estimator.processIMU()，将他们作为参数传进processIMU()函数进行操作。<br>这里有一点要注意，往processIMU()传数据时，imu时间戳大于或等于图像时间戳的那个数据要进行一下简单的线性分配。</p>
<pre><code>//对于大多数情况，IMU的时间戳都会比img的早，此时直接选取IMU的数据就行
if (t &lt;= img_t)  //http://wiki.ros.org/sensor_msgs
&#123;   
    if (current_time &lt; 0)
        current_time = t;
    double dt = t - current_time;
    ROS_ASSERT(dt &gt;= 0);
    current_time = t;
    dx = imu_msg-&gt;linear_acceleration.x;
    dy = imu_msg-&gt;linear_acceleration.y;
    dz = imu_msg-&gt;linear_acceleration.z;
    rx = imu_msg-&gt;angular_velocity.x;
    ry = imu_msg-&gt;angular_velocity.y;
    rz = imu_msg-&gt;angular_velocity.z;
    //这里干了2件事，IMU粗略地预积分，然后把值传给一个新建的IntegrationBase对象
    estimator.processIMU(dt, Vector3d(dx, dy, dz), Vector3d(rx, ry, rz));//进行IMU预积分
&#125; 
//对于处于边界位置的IMU数据，是被相邻两帧共享的，而且对前一帧的影响会大一些，在这里，对数据线性分配
else//每个大于图像帧时间戳的第一个imu_msg是被两个图像帧共用的(出现次数少)
&#123;
    double dt_1 = img_t - current_time; //current_time &lt; img_time &lt; t
    double dt_2 = t - img_t;
    current_time = img_t;
    ROS_ASSERT(dt_1 &gt;= 0);
    ROS_ASSERT(dt_2 &gt;= 0);
    ROS_ASSERT(dt_1 + dt_2 &gt; 0);
    //以下操作其实就是简单的线性分配
    double w1 = dt_2 / (dt_1 + dt_2);
    double w2 = dt_1 / (dt_1 + dt_2);
    dx = w1 * dx + w2 * imu_msg-&gt;linear_acceleration.x;
    dy = w1 * dy + w2 * imu_msg-&gt;linear_acceleration.y;
    dz = w1 * dz + w2 * imu_msg-&gt;linear_acceleration.z;
    rx = w1 * rx + w2 * imu_msg-&gt;angular_velocity.x;
    ry = w1 * ry + w2 * imu_msg-&gt;angular_velocity.y;
    rz = w1 * rz + w2 * imu_msg-&gt;angular_velocity.z;
    estimator.processIMU(dt_1, Vector3d(dx, dy, dz), Vector3d(rx, ry, rz));
&#125;
</code></pre><hr>
<h3 id="void-Estimator-processIMU"><a href="#void-Estimator-processIMU" class="headerlink" title="void Estimator::processIMU()"></a>void Estimator::processIMU()</h3><p>首先判断传来的imu是不是整个系统下的第一个imu数据，如果是，则要先给acc_0和gyr_0赋值。然后，判断pre_integrations[frame_count]是否为空，若为空，则new一个,主要操作就是给pre_integrations[frame_count]的成员参数acc_0, gyr_0，linearized_ba, linearized_bg赋值,未初始化时，linearized_ba, linearized_bg的赋值其实都为零。<br>这里的<code>pre_integrations[]</code>就是estimator的成员变量，数组大小为10，就是滑动窗口大小，在estimator的构造函数中，<code>pre_integrations[]</code>中的所有数据都先置为空。<code>pre_integrations[]</code>中的每个数据都是IntegrationBase类型的，这个类型的数据用来存储两帧之间的所有imu数据，预积分值，预积分累计误差的协方差以及残差的jacobian。<br>然后判断frame_count是否为0，若为0则不用进行预积分，直接更新acc_0和gyr_0就结束了，frame_count增加的操作在estimator.processImage()中。<br>若frame_count不为0，则进行预积分，关于预积分的操作从下面这句展开：</p>
<pre><code>pre_integrations[frame_count]-&gt;push_back(dt, linear_acceleration, angular_velocity);
</code></pre><p>这个push_back()是pre_integrations[frame_count]的函数，即IntegrationBase类的成员函数，该函数先把时差，加速度，角速度存到pre_integrations[frame_count]的成员变量dt_buf，acc_buf，gyr_buf中，然后调用函数propagate(dt, acc, gyr)，这个函数也是IntegrationBase类的成员函数，这个函数就放在下面讲了。<br>接下来，把预积分数据也存到tmp_pre_integration中，执行完processIMU()后，下面processImage()函数里，要用tmp_pre_integration给imageframe.pre_integration赋值。</p>
<pre><code>tmp_pre_integration-&gt;push_back(dt, linear_acceleration, angular_velocity);
</code></pre><p>然后就是一些参数存储和更新的工作了：</p>
<pre><code>dt_buf[frame_count].push_back(dt);
linear_acceleration_buf[frame_count].push_back(linear_acceleration);
angular_velocity_buf[frame_count].push_back(angular_velocity);

int j = frame_count;         
Vector3d un_acc_0 = Rs[j] * (acc_0 - Bas[j]) - g;
Vector3d un_gyr = 0.5 * (gyr_0 + angular_velocity) - Bgs[j];
Rs[j] *= Utility::deltaQ(un_gyr * dt).toRotationMatrix();
Vector3d un_acc_1 = Rs[j] * (linear_acceleration - Bas[j]) - g;
Vector3d un_acc = 0.5 * (un_acc_0 + un_acc_1);
Ps[j] += dt * Vs[j] + 0.5 * dt * dt * un_acc;
Vs[j] += dt * un_acc;

acc_0 = linear_acceleration;
gyr_0 = angular_velocity;
</code></pre><hr>
<h3 id="void-propagate"><a href="#void-propagate" class="headerlink" title="void propagate()"></a>void propagate()</h3><p>首先，通过传来的参数，给pre_integrations[frame_count]的成员变量dt, acc_1, gyr_1赋值，再创建一系列变量：</p>
<pre><code>Vector3d result_delta_p;
Quaterniond result_delta_q;
Vector3d result_delta_v;
Vector3d result_linearized_ba;
Vector3d result_linearized_bg;
</code></pre><p>再调用函数</p>
<pre><code>midPointIntegration(_dt, acc_0, gyr_0, _acc_1, _gyr_1, delta_p, delta_q, delta_v,
                        linearized_ba, linearized_bg,
                        result_delta_p, result_delta_q, result_delta_v,
                        result_linearized_ba, result_linearized_bg, 1);
</code></pre><p>这个函数的结果就是保存在上面创建的变量里，下面细看这个函数，midPointIntegration()总体分为两部分，一部分是预积分：</p>
<pre><code>Vector3d un_acc_0 = delta_q * (_acc_0 - linearized_ba);
Vector3d un_gyr = 0.5 * (_gyr_0 + _gyr_1) - linearized_bg;
result_delta_q = delta_q * Quaterniond(1, un_gyr(0) * _dt / 2, un_gyr(1) * _dt / 2, un_gyr(2) * _dt / 2);
Vector3d un_acc_1 = result_delta_q * (_acc_1 - linearized_ba);
Vector3d un_acc = 0.5 * (un_acc_0 + un_acc_1);
result_delta_p = delta_p + delta_v * _dt + 0.5 * un_acc * _dt * _dt;
result_delta_v = delta_v + un_acc * _dt;
result_linearized_ba = linearized_ba;
result_linearized_bg = linearized_bg;
</code></pre><p>这里对应的就是预积分的离散形式，具体公式如下图所示：<img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/预积分.png" alt="预积分"><br>$a^{b_k}$这种就是imu的加速计测量值，是在自身载体系下的加速度，所以外面会乘$q_{b_ib_{k+1}}$转到第i帧的坐标系下。</p>
<p><code>result_delta_q</code>，<code>result_delta_p</code>,<code>result_delta_v</code>,<code>result_linearized_ba</code>,<code>result_linearized_bg</code>就是对应着图中蓝色部分和最后两个，其中<code>result_linearized_ba</code>,<code>result_linearized_bg</code>在未初始化时，都是0，之后会重新预积分的。</p>
<p>第二部分是计算累计误差的协方差和残差关于ba、bg的Jacobian。</p>
<pre><code>MatrixXd F = MatrixXd::Zero(15, 15);
F.block&lt;3, 3&gt;(0, 0) = Matrix3d::Identity();
F.block&lt;3, 3&gt;(0, 3) = -0.25 * delta_q.toRotationMatrix() * R_a_0_x * _dt * _dt + 
                        -0.25 * result_delta_q.toRotationMatrix() * R_a_1_x * (Matrix3d::Identity() - R_w_x * _dt) * _dt * _dt;
F.block&lt;3, 3&gt;(0, 6) = MatrixXd::Identity(3,3) * _dt;
F.block&lt;3, 3&gt;(0, 9) = -0.25 * (delta_q.toRotationMatrix() + result_delta_q.toRotationMatrix()) * _dt * _dt;
F.block&lt;3, 3&gt;(0, 12) = -0.25 * result_delta_q.toRotationMatrix() * R_a_1_x * _dt * _dt * -_dt;
F.block&lt;3, 3&gt;(3, 3) = Matrix3d::Identity() - R_w_x * _dt;
F.block&lt;3, 3&gt;(3, 12) = -1.0 * MatrixXd::Identity(3,3) * _dt;
F.block&lt;3, 3&gt;(6, 3) = -0.5 * delta_q.toRotationMatrix() * R_a_0_x * _dt + 
                        -0.5 * result_delta_q.toRotationMatrix() * R_a_1_x * (Matrix3d::Identity() - R_w_x * _dt) * _dt;
F.block&lt;3, 3&gt;(6, 6) = Matrix3d::Identity();
F.block&lt;3, 3&gt;(6, 9) = -0.5 * (delta_q.toRotationMatrix() + result_delta_q.toRotationMatrix()) * _dt;
F.block&lt;3, 3&gt;(6, 12) = -0.5 * result_delta_q.toRotationMatrix() * R_a_1_x * _dt * -_dt;
F.block&lt;3, 3&gt;(9, 9) = Matrix3d::Identity();
F.block&lt;3, 3&gt;(12, 12) = Matrix3d::Identity();
//cout&lt;&lt;&quot;A&quot;&lt;&lt;endl&lt;&lt;A&lt;&lt;endl;

MatrixXd V = MatrixXd::Zero(15,18);
V.block&lt;3, 3&gt;(0, 0) =  0.25 * delta_q.toRotationMatrix() * _dt * _dt;
V.block&lt;3, 3&gt;(0, 3) =  0.25 * -result_delta_q.toRotationMatrix() * R_a_1_x  * _dt * _dt * 0.5 * _dt;
V.block&lt;3, 3&gt;(0, 6) =  0.25 * result_delta_q.toRotationMatrix() * _dt * _dt;
V.block&lt;3, 3&gt;(0, 9) =  V.block&lt;3, 3&gt;(0, 3);
V.block&lt;3, 3&gt;(3, 3) =  0.5 * MatrixXd::Identity(3,3) * _dt;
V.block&lt;3, 3&gt;(3, 9) =  0.5 * MatrixXd::Identity(3,3) * _dt;
V.block&lt;3, 3&gt;(6, 0) =  0.5 * delta_q.toRotationMatrix() * _dt;
V.block&lt;3, 3&gt;(6, 3) =  0.5 * -result_delta_q.toRotationMatrix() * R_a_1_x  * _dt * 0.5 * _dt;
V.block&lt;3, 3&gt;(6, 6) =  0.5 * result_delta_q.toRotationMatrix() * _dt;
V.block&lt;3, 3&gt;(6, 9) =  V.block&lt;3, 3&gt;(6, 3);
V.block&lt;3, 3&gt;(9, 12) = MatrixXd::Identity(3,3) * _dt;
V.block&lt;3, 3&gt;(12, 15) = MatrixXd::Identity(3,3) * _dt;

//step_jacobian = F;
//step_V = V;
jacobian = F * jacobian;
covariance = F * covariance * F.transpose() + V * noise * V.transpose();
</code></pre><p>先简单介绍一下其原理：<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/协方差1.png" alt="协方差1"><br>把上述公式套到imu的误差传递中，可得到下面这个式子：<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/协方差5.png" alt="协方差5"><br>这里我们得到了当前imu数据与上一个imu数据误差的线性关系，两帧的协方差可以根据这种线性关系递推得到：<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/协方差2.png" alt="协方差2"><br>这里的${\sum _i}_{k - 1}$就是上一个imu数据协方差，${\sum _n}$是噪声矩阵，在构造函数中通过从yaml文件中读取的噪声标准差数据赋了值。<br>由上所述，我们现在要求的就是矩阵F,G，结果如下：<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/协方差3.png" alt="协方差3"><br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/VINSMONO/协方差4.png" alt="协方差4"><br>具体的运算过程太长了，可以去看深蓝学院的ppt，这里的求解内容就存在了代码中的矩阵F和V中，然后用这两个矩阵完成对协方差和雅可比的更新。<br>协方差可用于预积分非线性优化时，转换成信息矩阵。这里的雅可比，只对残差对$b_a,b_g$的雅可比进行了更新，初始化的时候会用到，非线性优化时，会把所有雅可比都计算出来，下面贴一下协方差和雅可比矩阵的图，以便对它们有个基础认知。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/09/04/VINSMONO-NOTE-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/09/04/VINSMONO-NOTE-1/" itemprop="url">VINS-MONO源码阅读:前端</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2023-09-04T13:04:24+08:00">
                2023-09-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="VINSMONO学习笔记-1"><a href="#VINSMONO学习笔记-1" class="headerlink" title="VINSMONO学习笔记(1)"></a>VINSMONO学习笔记(1)</h1><p>这篇博客开始是对VINSMONO框架的源码学习，最近把深蓝学院的VIO课程看完了，希望通过阅读VINSMONO的源码来进行巩固和加深理解。这篇博客主要是对VINSMONO的整体框架的梳理和对前端节点代码的阅读。</p>
<p>参考博客：<a target="_blank" rel="noopener" href="https://blog.csdn.net/mcw1234/article/details/83039506">VINS-Mono详解（一）</a>，<a target="_blank" rel="noopener" href="https://blog.csdn.net/wbzhang233/article/details/88885485">港科大VINS-MONO入门（一）：框架入门及源码解析</a>，<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_51547017/article/details/123306540">VINS_mono 代码详细理解（一）</a></p>
<hr>
<h2 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h2><p>VINS-Mono是基于ros开发的，主要包含两个节点: 前端节点<code>feature_tracker_node</code>和后端节点<code>estimator_node</code>。前端节点处理Measurement Preprocessing中的Feature Detection and Tracking, 其他几个部分(IMU preintegration, initialization, LocalBA, Loop closure)都是在estimator_node中处理。 </p>
<p>前端节点的feature_tracker_node的程序入口是文件feature_tracker_node.cpp中的main()函数，接收图像信息,调用img_callback()函数进行处理。</p>
<p>estimator_node节点中开了3个子线程, 分别是:process,loop_detection和pose_graph. 其中 process()线程处理VIO后端, 包括(IMU preintegration, initialization, LocalBA)loop_detection()线程处理闭环检测pose_graph()线程分别处理全局优化。如果不需要做loop closure, 可以把LOOP_CLOUSRE置0, 则后面两个线程不会创建。</p>
<hr>
<h2 id="源码阅读"><a href="#源码阅读" class="headerlink" title="源码阅读"></a>源码阅读</h2><h3 id="int-main"><a href="#int-main" class="headerlink" title="int main()"></a>int main()</h3><p>首先，读取yaml文件中的相关参数，再创建由FeatureTracker类的实例组成的数组<code>trackerData[NUM_OF_CAM]</code>，其中<code>NUM_OF_CAM</code>为相机的个数，这意味这每一个相机都有一个FeatureTracker的实例，每个相机的FeatureTracker实例通过调用成员函数FeatureTracker::readIntrinsicParameter()，来读取每个相机各自对应的内参。</p>
<p>特别的，如果相机是鱼眼相机，需要读取FISHEYE_MASK，存到相机FeatureTracker的实例的成员变量fisheye_mask中，它会在后续操作中被用来去除边缘噪点。</p>
<p>接着定义一个订阅器和两个发布器。订阅器sub_img从话题IMAGE_TOPIC中订阅相机图像数据，回调函数为img_callback()。发布器pub_img在名为feature的话题下发布一条类型为sensor_msgs::PointCloud的消息，该话题消息为从相机图像中跟踪的特征点。发布器pub_match在名为feature_img的话题下发布一条类型为sensor_msgs::Image的消息，该话题消息为标记出了特征点的图像。<br>| |name|topic|type|消息内容|<br>|-|:———-:|:———:|:———:|:———:|<br>|subscriber|sub_img|IMAGE_TOPIC|sensor_msgs::Image|相机图像数据|<br>|publisher|pub_img|feature|sensor_msgs::PointCloud|跟踪的特征点|<br>|publisher|pub_match|feature_img|sensor_msgs::Image|标记出了特征点的图像|</p>
<hr>
<h3 id="void-img-callback"><a href="#void-img-callback" class="headerlink" title="void img_callback()"></a>void img_callback()</h3><p>每当接收到从IMAGE_TOPIC话题订阅的数据，就会进入回调函数img_callback()进行处理。对于传来的图像，首先要判断其是否需要发送，并不是每处理一帧图像，都将特征点检测跟踪结果发布出去。数据发布频率由配置参数FREQ给定，通过PUB_THIS_FRAME控制是否发布当前帧的检测跟踪数据，将数据平均发布频率稳定在FREQ：如果当前统计时间内的平均数据发布频率快于FREQ，则将PUB_THIS_FRAME置为false，只进行特征点的跟踪，但不发布当前帧的数据；否则，将PUB_THIS_FRAME置为true，进行特征点的跟踪且发布当前帧的数据。</p>
<p>然后遍历成员变量<code>trackerData</code>数组，调用该对象的函数readImage()，将数据读取存储到<code>trackerData</code>里(这部分下面会详细讲)。然后更新<code>trackerData</code>里特征点的id。</p>
<p>接下来，创建了一个名为 <code>feature_points</code> 的指针，该指针指向一个 sensor_msgs::PointCloud 类型的消息对象,将<strong>当前帧的所有特征点在归一化平面上的坐标</strong>存到<code>feature_points</code>的成员变量<code>points</code>中，将<strong>当前帧的所有特征点的像素坐标，id，xy轴方向的速度</strong>存到<code>feature_points</code>的成员变量<code>channels</code>中，通过<code>pub_img</code>发布出去（第一帧不发送）。</p>
<p>最后，如果要进行特征点轨迹的可视化，即SHOW_TRACK为true，则将原来转为灰度图的图像重新恢复成彩色图，存到变量stereo_img里，然后按图像大小截取到tmp_img里，在tmp_img中特征点坐标处画圆圈，颜色深度由追踪到的次数决定，通过<code>pub_match</code>将处理过的图像发布出去。</p>
<hr>
<h3 id="void-FeatureTracker-readImage"><a href="#void-FeatureTracker-readImage" class="headerlink" title="void FeatureTracker::readImage()"></a>void FeatureTracker::readImage()</h3><p>FeatureTracker类中的主要处理函数就是readImage()，在这个函数中涉及到几个变量名，需要对它们的含义进行特别说明（以下说明针对单目模式，其含义并不适用于双目模式），否则根据变量名称去揣测其含义会出错。</p>
<p>图像数据变量：</p>
<p><code>prev_img</code>： 上一次发布数据时对应的图像帧<br><code>cur_img</code>： 光流跟踪的前一帧图像，而不是“当前帧”<br><code>forw_img</code>： 光流跟踪的后一帧图像，真正意义上的“当前帧”特征点数据变量：</p>
<p><code>prev_pts</code>： 上一次发布的，且能够被当前帧（forw）跟踪到的特征点<br><code>cur_pts</code>： 在光流跟踪的前一帧图像中，能够被当前帧（forw）跟踪到的特征点<br><code>forw_pts</code>： 光流跟踪的后一帧图像，即当前帧中的特征点（除了跟踪到的特征点，可能还包含新检测的特征点）</p>
<p>函数的具体流程为：<br>1.如果控制参数EQUALIZE为true，调用cv::createCLAHE对图像进行自适应直方图均衡处理；</p>
<p>2.调用cv::calcOpticalFlowPyrLK()对前一帧的特征点cur_pts进行金字塔光流跟踪，得到forw_pts。status标记了cur_pts中各个特征点的跟踪状态，根据status将跟踪失败的特征点从prev_pts、cur_pts和forw_pts中剔除，而且在记录特征点id的ids，和记录特征点被跟踪次数的track_cnt中，也要把这些跟踪失败的特征点对应位置的记录删除。被status标记为跟踪正常的特征点，在当前帧图像中的位置可能已经处于图像边界外了，这些特征点也应该被删除，删除操作同上。最后将未被删除的点的跟踪次数加1。</p>
<p>3.如果不需要发布当前帧的数据，则直接将当前帧forw的相关数据赋给上一帧cur，然后在这一步整个readImage的流程就结束了。</p>
<p>4.如果需要发布当前帧的数据，先调用FeatureTracker::rejectWithF()函数，剔除outliers。具体方法为：调用cv::findFundamentalMat()对prev_pts和forw_pts计算F矩阵，通过F矩阵去除outliers。剩下的特征点track_cnt都加一。</p>
<p>5.调用FeatureTracker::setMask()，通过设置一个mask，使跟踪的特征点在整幅图像中能够均匀分布，防止特征点扎堆。FeatureTracker::setMask()的具体操作为：对光流跟踪到的特征点forw_pts，按照被跟踪到的次数降序排列，然后按照降序遍历这些特征点。每选中一个特征点，在mask中将该点周围半径为MIN_DIST的区域设置为30，后面不再选取该区域内的特征点。这样会删去一些特征点，使得特征点分布得更加均匀，同时尽可能地保留被跟踪次数更多的特征点。</p>
<p>6.由于光流跟踪到的特征点会减少，而且setMask()的处理过程中也会删除一些特征点，所以需要新检测一些特征点（只有需要发布数据时，才会检测新的特征点，否则只跟踪，不检测新的特征点）。具体操作为：调用cv::goodFeaturesToTrack()在mask中不为0的区域检测新的特征点，将特征点数量补充至指定数量。然后调用FeatureTracker::addPoints()，将新检测到的特征点到forw_pts中去，id初始化为-1，track_cnt初始化为1。</p>
<p>7.将cur的相关变量赋给prev的相关变量，forw的相关变量赋给cur的相关变量，调用函数undistortedPoints()，对当前帧的特征点去畸变，得到<code>cur_un_pts</code> 、<code>cur_un_pts_map</code> 、<code>prev_un_pts_map</code> ,并且求出每个特征点xy方向的速度。undistortedPoints()中调用了liftProjective()函数实现了去畸变，它用的方法和slam14讲里面的不太一样，它使用迭代畸变模型，循环迭代八次，这样效果比函数近视好，而速度也比opencv的API快。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/27/ORBSLAM3-NOTE-12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/04/27/ORBSLAM3-NOTE-12/" itemprop="url">ORB-SLAM3源码阅读:KeyFrameDatabase</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2023-04-27T15:31:18+08:00">
                2023-04-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="ORBSLAM3学习笔记-12"><a href="#ORBSLAM3学习笔记-12" class="headerlink" title="ORBSLAM3学习笔记(12)"></a>ORBSLAM3学习笔记(12)</h1><p>这篇博客主要是对KeyFrameDatabase.cc文件的学习，这部分的内容很少，原来不打算专门写一篇的，但写在别的地方又太乱了，还是在这里记录一下吧。</p>
<hr>
<h2 id="源码阅读"><a href="#源码阅读" class="headerlink" title="源码阅读"></a>源码阅读</h2><h3 id="KeyFrameDatabase-DetectRelocalizationCandidates"><a href="#KeyFrameDatabase-DetectRelocalizationCandidates" class="headerlink" title="KeyFrameDatabase::DetectRelocalizationCandidates()"></a>KeyFrameDatabase::DetectRelocalizationCandidates()</h3><ol>
<li>找出和当前帧具有公共单词的所有关键帧<blockquote>
<p>这里是通过遍历成员变量mvInvertedFile得到的，mvInvertedFile就是存储每个单词所有的关键帧的。把关键帧添加到mvInvertedFile的步骤是在闭环线程进行的。</p>
</blockquote>
</li>
<li>统计上述关键帧中与当前帧F具有共同单词最多的单词数，用来设定阈值1<blockquote>
<p>阈值1是最小公共单词数为最大公共单词数目的0.8倍</p>
</blockquote>
</li>
<li>遍历上述关键帧，挑选出共有单词数大于阈值1的及其和当前帧单词匹配得分存入lScoreAndMatch<blockquote>
<p>这里的得分是通过DBoW2库的score()函数计算得来的，用mBowVec来计算两帧的相似度得分，具体代码就先不看了。</p>
</blockquote>
</li>
<li>计算lScoreAndMatch中每个关键帧的共视关键帧组的总得分，得到最高组得分bestAccScore，并以此决定阈值2<blockquote>
<p>注意，这里将与关键帧共视程度最高的前十个关键帧归为一组，计算累计得分。阈值2是最高得分组的0.75倍</p>
</blockquote>
</li>
<li>得到所有组中总得分大于阈值2的，组内得分最高的关键帧，作为候选关键帧组。</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/13/ORBSLAM3-NOTE-11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/04/13/ORBSLAM3-NOTE-11/" itemprop="url">ORB-SLAM3源码阅读:MapPoint</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2023-04-13T19:29:38+08:00">
                2023-04-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="ORBSLAM3学习笔记-11"><a href="#ORBSLAM3学习笔记-11" class="headerlink" title="ORBSLAM3学习笔记(11)"></a>ORBSLAM3学习笔记(11)</h1><p>这篇博客主要是对MapPoint.cc文件的学习。</p>
<hr>
<h2 id="源码阅读"><a href="#源码阅读" class="headerlink" title="源码阅读"></a>源码阅读</h2><h3 id="MapPoint-AddObservation"><a href="#MapPoint-AddObservation" class="headerlink" title="MapPoint::AddObservation()"></a>MapPoint::AddObservation()</h3><p>这个函数引用的参数为(KeyFrame* pKF, int idx),及关键帧与特征点索引值，功能是为MapPoint的成员变量mObservations赋值，所以我们先详细看一下这个变量就行。<br>首先，它的类型为std::map<KeyFrame\*,std::tuple<int,int>&gt; 这是一个map容器，键值为我们传过来的KeyFrame，value是一个元组类型std::tuple&lt; int,int&gt;，由于我们看的是单目，其实我们只用得到元组的第一个位置，存储的是该MapPoint在该关键帧的投影特征点的索引值。<br>最后还要更新成员int nObs，该成员记录了当前地图点被多少个关键帧相机观测到了(单目关键帧每次观测算1个相机,双目/RGBD帧每次观测算2个相机)。</p>
<hr>
<h3 id="MapPoint-UpdateNormalAndDepth"><a href="#MapPoint-UpdateNormalAndDepth" class="headerlink" title="MapPoint::UpdateNormalAndDepth()"></a>MapPoint::UpdateNormalAndDepth()</h3><p>该函数的功能是更新地图点平均观测方向和观测距离范围。首先看一下观测距离范围是什么意思，这部分就参考博客<a target="_blank" rel="noopener" href="https://blog.csdn.net/ncepu_Chen/article/details/116784652?spm=1001.2014.3001.5502">ORB-SLAM2代码详解03: 地图点MapPoint</a>，然后在看具体代码。</p>
<p><strong>基本概念</strong><br>MapPoint类中观测距离范围的由成员变量mfMaxDistance和mfMinDistance表示。</p>
<ul>
<li><code>mfMaxDistance</code>表示若地图点匹配在某特征提取器图像金字塔第7层上的某特征点,观测距离值</li>
<li><code>mfMinDistance</code>表示若地图点匹配在某特征提取器图像金字塔第0层上的某特征点,观测距离值<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/orbslam3/观测尺度1.png" alt="观测尺度"><br>这里的dist是由参考关键帧得到的，参考关键帧就是创建MapPoint对象时传递过去的KeyFrame参数。dist与mfMaxDistance，mfMinDistance之间的换算关系为<script type="math/tex">\frac{mfMaxDistance}{dist}\text=1.2^{level},\frac{dist}{mfMinDistance}\text=1.2^{(n-1-\text{level})}</script> $level$是MapPoint在参考关键帧中对应关键点的金字塔层数，$n$是金字塔的总层数。</li>
</ul>
<p><strong>平均观测方向</strong><br>遍历每个观测到该地图点的关键帧，求他们光心到该地图点的向量，即观测方向，光心就是该关键帧的位姿变换的平移部分。然后将该向量归一化，将所有向量加起来除以向量个数。</p>
<p><strong>观测距离范围</strong><br>得到参考关键帧(即地图点第一次创建时的关键帧)与地图点之间的距离</p>
<pre><code>Eigen::Vector3f PC = Pos - pRefKF-&gt;GetCameraCenter(); 
const float dist = PC.norm(); 
</code></pre><p>然后根据上面的公式求得最大距离和最小距离。</p>
<p>最后，该函数就是把平均观测方向存入mNormalVector中，把最大和最小距离存入mfMaxDistance和mfMinDistance中。</p>
<p><strong>函数MapPoint::UpdateNormalAndDepth()的调用时机:</strong></p>
<ul>
<li>创建地图点时调用该函数初始化其观测信息。</li>
<li>地图点对关键帧的观测<code>mObservations</code>更新时(<strong>跟踪局部地图添加或删除对关键帧的观测</strong>时、<strong>LocalMapping线程删除冗余关键帧</strong>时或<strong>LoopClosing线程闭环矫正</strong>时),调用该函数初始化其观测信息。</li>
<li>地图点世界坐标mWorldPos发生变化时(BA优化之后),调用该函数初始化其观测信息.</li>
</ul>
<p>总结成一句话: 只要地图点本身或关键帧对该地图点的观测发生变化,就应该调用该函数更新其观测尺度和方向信息。</p>
<hr>
<h3 id="MapPoint-Replace"><a href="#MapPoint-Replace" class="headerlink" title="MapPoint::Replace()"></a>MapPoint::Replace()</h3><p>函数的调用形式为pMPinKF-&gt;Replace(pMP)。将pMPinKF地图点替换成pMP。下面看具体步骤：</p>
<ol>
<li>先将mMutexFeatures，mMutexPos上锁(这里是避免track线程里获取mpReplaced时冲突)，将pMP赋值给mpReplaced;</li>
<li>我们先把pMPinKF的<code>mObservations</code>拷贝到obs，再将<code>mObservations</code>清空。</li>
<li>遍历obs，每次拿出一组关键帧pKF和对应索引值indexes，判断传过来的pMP是否在pKF中：</li>
</ol>
<ul>
<li>若不在，把pMP存到pKF的<code>mvpMapPoints</code>的indexes的位置，这里本来存储的是pMPinKF，这一步就是用pMP把pMPinKF覆盖了。再把pKF添加到pMP的<code>mObservations</code>中。</li>
<li>若存在，我们直接把pKF中的pMPinKF删掉就行。</li>
</ul>
<ol>
<li>将当前地图点的观测数据等其他数据都”叠加”到新的地图点上，包括nfound，nvisible，更新地图点描述子，删掉地图中的pMPinKF。</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/13/ORBSLAM3-NOTE-10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/04/13/ORBSLAM3-NOTE-10/" itemprop="url">ORB-SLAM3源码阅读:KeyFrame</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2023-04-13T19:26:34+08:00">
                2023-04-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="ORBSLAM3学习笔记-10"><a href="#ORBSLAM3学习笔记-10" class="headerlink" title="ORBSLAM3学习笔记(10)"></a>ORBSLAM3学习笔记(10)</h1><p>这篇博客主要是对KeyFrame.cc文件的学习。</p>
<hr>
<h2 id="源码阅读"><a href="#源码阅读" class="headerlink" title="源码阅读"></a>源码阅读</h2><h3 id="KeyFrame-KeyFrame"><a href="#KeyFrame-KeyFrame" class="headerlink" title="KeyFrame::KeyFrame()"></a>KeyFrame::KeyFrame()</h3><p>就是通过传来的参数给相应的成员变量赋值。传来的pMap对象是指针型的，故KeyFrame类对象的成员变量mpMap指向的是Atlas类对象的成员变量mpCurrentMap。</p>
<hr>
<h3 id="KeyFrame-ComputeBoW"><a href="#KeyFrame-ComputeBoW" class="headerlink" title="KeyFrame::ComputeBoW()"></a>KeyFrame::ComputeBoW()</h3><p>这里就是给mBowVec和mFeatVec两个成员变量赋值。这里可以直接看<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41694024/article/details/126328833">ORB-SLAM2 —— Frame::ComputeBoW函数</a>,这篇博客讲得细致清晰，我就不多做赘述了，就直接放下源码注释。</p>
<pre><code>void KeyFrame::ComputeBoW()
&#123;
    // 只有当词袋向量或者节点和特征序号的特征向量为空的时候执行
    if (mBowVec.empty() || mFeatVec.empty())
    &#123;
        // 将描述子mDescriptors转换为DBOW要求的输入格式
        vector&lt;cv::Mat&gt; vCurrentDesc = Converter::toDescriptorVector(mDescriptors);

        // 将特征点的描述子转换成词袋向量mBowVec以及特征向量mFeatVec
        mpORBvocabulary-&gt;transform(vCurrentDesc,    //当前的描述子vector
                               mBowVec,         //输出，词袋向量，记录的是单词的id及其对应权重TF-IDF值
                               mFeatVec,        //输出，记录node id及其对应的图像 feature对应的索引
                               4);              //4表示从叶节点向前数的层数
    &#125;
&#125;
</code></pre><p>关于mBowVec和mFeatVec的具体类型，看以下注释。</p>
<pre><code>// Bag of Words Vector structures.
// 内部实际存储的是std::map&lt;WordId, WordValue&gt;
// WordId 和 WordValue 表示Word在叶子中的id 和权重
DBoW2::BowVector mBowVec;
// 内部实际存储 std::map&lt;NodeId, std::vector&lt;unsigned int&gt; &gt;
// NodeId 表示节点id，std::vector&lt;unsigned int&gt; 中实际存的是该节点id下所有特征点在图像中的索引
DBoW2::FeatureVector mFeatVec;
</code></pre><hr>
<h3 id="KeyFrame-UpdateConnections"><a href="#KeyFrame-UpdateConnections" class="headerlink" title="KeyFrame::UpdateConnections()"></a>KeyFrame::UpdateConnections()</h3><p>1.创建map<KeyFrame *, int>类变量KFcounter，键是该关键帧的共视关键帧，值是该这两个关键帧的共视点数目。</p>
<blockquote>
<p>具体做法是先获得该关键帧的所有地图点。遍历地图点，根据地图点的成员变量mObservations，往KFcounter中添加键值对。</p>
</blockquote>
<p>2.创建vector<pair<int, KeyFrame *>&gt;类变量vPairs，这里面存储与该关键帧共视点超过15的关键帧。</p>
<blockquote>
<p>这步就是遍历KFcounter，把达到要求的共视关键帧以及共视点数目放入vPairs中。同时调用AddConnection(),给这个共视关键帧更新连接及权重。<br>AddConnection()就是为std::map<KeyFrame*,int>类成员变量<code>mConnectedKeyFrameWeights</code>赋值，之后调用UpdateBestCovisibles()，对连接关系按照权重(即共视点数目)从大到小排序，把排完序的关键帧和权重存入<code>mvpOrderedConnectedKeyFrames</code>和<code>mvOrderedWeights</code>中。</p>
</blockquote>
<p>3 如果没有连接到关键帧（超过阈值的权重），则对权重最大的关键帧建立连接，同时调用AddConnection(),给这个权重最大的关键帧更新连接及权重。<br>4.对共视程度比较高的关键帧对更新连接关系及权重</p>
<blockquote>
<p>前面我们已经把共视程度比较高的关键帧存入vPairs中了，也为这些关键帧更新了连接关系及权重，但我们自己还没更新。这步就是给自己更新的，操作和上面差不多，排好序后，存入成员变量<code>mvpOrderedConnectedKeyFrames</code>和<code>mvOrderedWeights</code>中。</p>
</blockquote>
<p>5.更新生成树的连接</p>
<blockquote>
<p>将该关键帧的成员变量<code>mpParent</code>指向共视程度最高的那个关键帧，得到了该关键帧的父关键帧。<br>mpParent调用函数AddChild(),在这个父关键帧的成员变量<code>mspChildrens</code>容器中添加了一个关键帧，即我们目前的关键帧，表明这个关键帧是父关键帧的子关键帧。</p>
</blockquote>
<p><strong>总结</strong><br>该函数为调用该函数的关键帧的成员变量<code>mvpOrderedConnectedKeyFrames</code>和<code>mvOrderedWeights</code>赋值，里面保存了共视程度比较高的关键帧和权重。同时，也在这些共视程度比较高的关键帧的<code>mvpOrderedConnectedKeyFrames</code>和<code>mvOrderedWeights</code>中添加了该关键帧和对应权重。<br>最后，将<code>mpParent</code>指向共视程度最高的那个关键帧，也在那个共视程度最高的那个关键帧的<code>mspChildrens</code>中添加了该关键帧。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/11/ORBSLAM3-NOTE-9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/04/11/ORBSLAM3-NOTE-9/" itemprop="url">ORB-SLAM3源码阅读:TwoViewReconstruction</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2023-04-11T13:53:20+08:00">
                2023-04-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="ORBSLAM3学习笔记-9"><a href="#ORBSLAM3学习笔记-9" class="headerlink" title="ORBSLAM3学习笔记(9)"></a>ORBSLAM3学习笔记(9)</h1><p>这篇博客主要是对TwoViewReconstruction.cc文件的学习。</p>
<hr>
<h2 id="一、基本原理"><a href="#一、基本原理" class="headerlink" title="一、基本原理"></a>一、基本原理</h2><p>这个文件用来完成单目初始化，里面涉及特征点坐标的归一化，对极约束求基础矩阵，求单应矩阵，RANSAC方法，双向重投影误差</p>
<hr>
<h3 id="1-特征点归一化"><a href="#1-特征点归一化" class="headerlink" title="1.特征点归一化"></a>1.特征点归一化</h3><p>矩阵A是利用8点法求基础矩阵的关键,所以Hartey就认为,利用8点法求基础矩阵不稳定的一个主要原因就是原始的图像像点坐标组成的系数矩阵A不好造成的,而造成A不好的原因是像点的齐次坐标各个分量的数量级相差太大。基于这个原因,Hartey提出一种改进的8点法,在应用8点法求基础矩阵之前,先对像点坐标进行归一化处理,即对原始的图像坐标做同向性变换,这样就可以减少噪声的干扰,大大的提高8点法的精度。</p>
<p>预先对图像坐标进行归一化有以下好处：<br>1.能够提高运算结果的精度。<br>2.利用归一化处理后的图像坐标,对任何尺度缩放和原点的选择是不变的。归一化步骤预先为图像坐标选择了一个标准的坐标系中,消除了坐标变换对结果的影响。</p>
<p>坐标归一化主要分为平移和缩放，具体步骤如下：<br><strong>平移</strong><br>首先求均值</p>
<script type="math/tex; mode=display">\begin{cases} x_{mean}=\frac{1}{N}\sum_{i}^{N}{p_x^i}\\ y_{mean}=\frac{1}{N}\sum_{i}^{N}{p_y^i} \end{cases}</script><p>得到均值后，用原坐标减去均值，这里的均值相当于新的坐标原点</p>
<script type="math/tex; mode=display">p_x^i=p_x^i-x_{mean},p_y^i=p_y^i-y_{mean}</script><p><strong>缩放</strong><br>首先求系数$s_x$和$s_y$，系数的几何意义就是单位距离。求法就是所有坐标到新原点($x_{mean},y_{mean}$)的距离之和的均值。<br>$\begin{cases} s_x = \frac{\sum\limits_{i = 0}^N {|p_x^i - x_{mean}|}}N\\s_y = \frac{\sum\limits_{i = 0}^N {|p_y^i - y_{mean}|} }N\end{cases}$<br>得到系数后，将平移后的坐标乘以单位距离，就得到归一化的坐标了。<br>$p_x^i=s_xp_x^i,p_y^i=s_yp_y^i$</p>
<p>这两个过程可以用一个变换T来描述</p>
<script type="math/tex; mode=display">p' = TpT=\left[ \begin{matrix}
   s_x & 0 & -{x_{mean}}s_x  \\
   0 & s_y & -{y_{mean}}s_y  \\
   0 & 0 & 1  \\
\end{matrix} \right]</script><p>这个T要保存一下，等会计算出来H、F，是归一化后的，以H为例，要用$T^{-1}HT$还原回来。</p>
<hr>
<h3 id="2-基础矩阵求解"><a href="#2-基础矩阵求解" class="headerlink" title="2.基础矩阵求解"></a>2.基础矩阵求解</h3><p>这一部分十四讲讲得还是比较细致的，图我就不放了，就直接进行一下运算推导吧<br>已知$p_1,p_2$是路标点$P$的像素坐标，得到$s_1p_1=KP,s_2p_2=K(RP+t)$,这里的$s_1,s_2$就是路标点的深度信息。现在取$x_1=K^{-1}p_1,x_2=K^{-1}p_2$显然，$x_1,x_2$是两个像素点的归一化平面上的坐标。这里我们不用尺度意义相等，直接代入上式，得\[s_2x_2=s_1R{x_1}+t\]然后两边同时与t做外积\[s_2t\hat{\ }x_2=s_1t\hat{\ }R{x_1}\]然后，两侧同时左乘$x_2^T$,由于左边的向量垂直于$x_2$，两个垂直向量的内积为0，故得到\[\frac{s_1}{s_2}x_2^Tt\hat{\ }Rx_1=0\]由这个式子可以看出，$s_1,s_2$无论取什么值都是没有意义的，取什么式子都成立，故路标点的深度信息缺失了，上式有尺度不确定性。<br>我们把$s_1,s_2$约掉，重新代入$p_1,p_2$得\[p_2^TK^{-1}t\hat{\ }RK^{-1}p_1=0 \]令$E=t\hat{\ }R,F=K^{-T}EK^{-1}$,E就是本质矩阵，F就是基础矩阵，下面求解基础矩阵。<br>展开成矩阵形式<script type="math/tex">\left( u_2,v_2,1 \right)\left( \begin{matrix}
   f_1 & f_2 & f_3  \\
   f_4 & f_5 & f_6  \\
   f_7 & f_8 & f_9  \\
\end{matrix} \right)\left( \begin{matrix}
   u_1  \\
   v_1  \\
   1  \\
\end{matrix} \right)=0</script>整理成$Ax=0$的形式:\[\left[ u_2u_1,u_2v_1,u_2v_2,u_1,v_2v_1,v_2,u_1,v_1,1 \right]f=0\]这里一个点对形成一个约束，$f$有9个变量，但$t\hat{\ }R$实际上只有6个自由度，再加上缺少深度信息，有尺度不确定性，其实只有5自由度。我们选择8个点对，求解出$f$的参数。<br>基础矩阵F可以做奇异值分解得\[F=UΣV^T\]我们需要根据$U,Σ,V$来求$R,t$,且$Σ=diag(σ,σ,0)$,这是本质矩阵的基本性质，但我们根据线性方程解出来的$F$很可能不满足该性质，因此一般都是$F$矩阵进行奇异值分解，强制令最小奇异值为0，然后重构回来得到$F$。<br>最后，关于我们求得的$R,t$尺度不确定性的问题，由于旋转矩阵$R$有其自身的约束，我们就认为这份不确定性在t上，为了解决这个问题，我们把t归一化，让它长度为一。这样，我们就把尺度定好了，接下来，我们就能用这个单位长度为1的t来求路标点深度，接下来的t也都以此为参考，这样我们初始化的目标就达到了。</p>
<hr>
<h3 id="3-单应矩阵求解"><a href="#3-单应矩阵求解" class="headerlink" title="3.单应矩阵求解"></a>3.单应矩阵求解</h3><p>单应矩阵的推导十四讲里写得很简略，我们这里先看图<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/orbslam3/%E5%8D%95%E5%BA%94%E7%9F%A9%E9%98%B5.png" alt="示例图"><br>上图表示场景中的平面$π$在两相机的成像，P是平面上的路标点(图片中是X)，设平面$π$在第一个相机坐标系下的单位法向量为$n$，平面到第一个相机中心（坐标原点）的距离为d，则平面$π$满足方程：\[n^TP=d\]这个式子还是好理解的，就是光心到P点的向量点乘法向量，就是$P$在法向量的投影长度乘以法向量单位长度1，很明显就是对于d。将上式变换为\[\frac1dn^TP=1\]把上式代入$p_2≃K(RP+t)$中，得到\[p_2≃K\left( RP+t\left( \frac{n^TP}d \right) \right)≃K\left( R-\frac{tn^T}d \right)P≃K\left( R-\frac{tn^T}d \right)K^{-1}p_1\]中间部分记为H，则\[p_2≃Hp_1\]将该式矩阵展开\[\left[ \begin{matrix}<br>   u_2  \\<br>   v_2  \\<br>   1  \\<br>\end{matrix} \right]≃\left[ \begin{matrix}<br>   h_1 &amp; h_2 &amp; h_3  \\<br>   h_4 &amp; h_5 &amp; h_6  \\<br>   h_7 &amp; h_8 &amp; h_9  \\<br>\end{matrix} \right]\left[ \begin{matrix}<br>   u_1  \\<br>   v_1  \\<br>   1  \\<br>\end{matrix} \right]\]分开写\[u_2=\frac{h_1u_1+h_2v_1+h_3}{h_7u_1+h_8v_1+h_9}\]\[v_2=\frac{h_4u_1+h_5v_1+h_6}{h_7u_1+h_8v_1+h_9}\]<br>整理成$Ax=0$的形式<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/orbslam3/6.png" alt="公式"><br>根据上面的式子可以看出一对匹配点可以构造两个约束，其实我们用四对点就能解了，但还是和上面保持一致，用8组点。对于最小二乘法问题$Ax=0$，有很多解法，orbslam3直接进行SVD分解，最小奇异值对应的奇异向量就是x的解。</p>
<hr>
<h3 id="4-RANSAC方法"><a href="#4-RANSAC方法" class="headerlink" title="4.RANSAC方法"></a>4.RANSAC方法</h3><p>RANSAC就是随机采样一致，简单理解为随机取一小部分样本计算模型，然后统计在该模型下的内点数（inliers），重复很多次，取内点数最多的那次的模型为最优结果。随机抽取一部分样本计算模型，而不是用整个样本计算，是因为样本中存在outliers，一起计算会影响结果。ransac寄希望于取的那小部分样本全都是优秀的inliers，这样算出来的模型就会是很好的，所以要重复很多次。<br>orbslam3用的是简化过的RANSAC方法，它就是每次去8个不同的点，计算H或F，然后用这个H或F对所有匹配点计算双向重投影误差，利用卡方检验，评判H或F好坏。重复若干次，取其中最好的H或F。<br>下面讲一下通常的RANSAC方法<br>1.随机的从S中选择s个数据点组成一个样本做为模型的一个示例。其中s是构建模型需要最少的点，如拟合直线s=2。<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/orbslam3/RANSAC1.png" alt="RANSAC1"><br>2.计算S中所有点与模型的距离，确定在模型距离阈值t内的数据点集Si，Si称为采样的一致集并定义为S的内点。<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/orbslam3/RANSAC2.png" alt="RANSAC2"><br>3.如果Si的大小（内点的数目）大于某个阈值T，用Si的所有点重新估计模型并结束。<br>4.如果Si的大小小于T，选择一个新的子集并重复1，2，3的过程。<br>5.经过N次试验选择最大一致集Si，并用Si所有点重新估计模型。<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/orbslam3/RANSAC3.png" alt="RANSAC2"></p>
<hr>
<h3 id="5-卡方检验"><a href="#5-卡方检验" class="headerlink" title="5.卡方检验"></a>5.卡方检验</h3><p>RANSAC迭代过程中，由于可能使用的点是外点，结果会有很大误差，我们要剔除这些结果，只留下最好的H或F。因此我们要去评估求得的结果的误差大小，而卡方检验正好满足我们的需求。<br>卡方检验我们需要了解三个东西卡方值，自由度，卡方检验表。首先是卡方值，我们先计算重投影误差，这也就是残差，而残差的平方除以随机项方差服从卡方分布，这是因为残差平方和中每一项都服从N（0,1）也就是标准正态分布，故他们之和服从卡方分布，这是卡方分布的基本定义。由此我们可以得出该问题的卡方值就是残差的平方除以随机项方差。然后根据自由度，查卡方检验表，得到阈值。比较卡方值和阈值，超出的我们就舍弃，在范围内的，我们就代入公式计算得分。<br>对于H\[p_2=H_{21}p_1,p_1=H_{12}p_2\]将$p_1$通过$H_{21}$投影得到$p_2’$，同理将 $p_2$通过$H_{12}$投影得到$p_1’$，卡方值就是$\frac{||p_1-p_1’||^2_2}{\sigma ^2}$和$\frac{||p_2-p_2’||^2_2}{\sigma ^2}$，其中$\sigma=1$,该模型下的自由度为2，因为计算的是坐标(u,v)的残差。取95%置信度下的自由度为2的卡方检验统计量阈值等于5.991。然后比较卡方值和阈值，若超过，则内点标志位置为false，反之，则用阈值减去卡方值，算入得分中。<br>对于F，残差其实是投影点到极线的距离。$Fp_1$得到的三个值a,b,c,很明显，$p_2$就在直线ax+by+c=0上，但由于误差存在，$p_2$位置肯定会有所偏移，由此计算点到线的距离为残差。这里的自由度为1(z这里没太搞懂)，取95%置信度下的自由度为1的卡方检验统计量阈值,然后和上面步骤差不多。</p>
<hr>
<h3 id="6-由F恢复R-t"><a href="#6-由F恢复R-t" class="headerlink" title="6.由F恢复R,t"></a>6.由F恢复R,t</h3><p>参考博客<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44580210/article/details/90344511">多视图几何总结——从本质矩阵恢复摄像机矩阵</a></p>
<hr>
<h3 id="8-三角化恢复3D坐标"><a href="#8-三角化恢复3D坐标" class="headerlink" title="8.三角化恢复3D坐标"></a>8.三角化恢复3D坐标</h3><p>一个3D点的齐次坐标为${\left[ x,y,z,1 \right]}^{T}$，它到图像上的投影为\[\lambda \left[ \begin{matrix}<br>   u  \\<br>   v  \\<br>   1  \\<br>\end{matrix} \right]=\underbrace{K\left[ R|t \right]}_{P}\left[ \begin{matrix}<br>   x  \\<br>   y  \\<br>   z  \\<br>   1  \\<br>\end{matrix} \right]\]\[\Downarrow \]\[\lambda \textbf{u}=\textbf{PX}\]两边同时左叉乘$\textbf{u}$,有\[\textbf{u}\hat{\ }\textbf{PX}=0\]展开后可得\[\left[ \begin{matrix}<br>   0 &amp; -1 &amp; v  \\<br>   1 &amp; 0 &amp; -u  \\<br>   -v &amp; u &amp; 0  \\<br>\end{matrix} \right]\left[ \begin{matrix}<br>   P_1  \\<br>   P_2  \\<br>   P_3  \\<br>\end{matrix} \right]X=0\]\[\Downarrow \]\[\left\{ \begin{matrix}<br>   (v{P_3}-{P_2})X=0  \\<br>   ({P_1}-u{P_3})X=0  \\<br>   (u{P_2}-v{P_1})X=0  \\<br>\end{matrix} \right.\]上面的方程很明显是线性相关的，实际上只有两个约束，但我们这里有两帧上的匹配点，由此可以得到两组方程即四个约束：<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/orbslam3/三角化.png" alt="三角化"><br>这里可以使用SVD求解，齐次坐标$\textbf{X}$即为$\textbf{H}$的最小奇异值的奇异向量。<br>这部分的代码实现在GeometricTools::Triangulate()函数中。</p>
<hr>
<h2 id="二、源码阅读"><a href="#二、源码阅读" class="headerlink" title="二、源码阅读"></a>二、源码阅读</h2><h3 id="TwoViewReconstruction-Reconstruct"><a href="#TwoViewReconstruction-Reconstruct" class="headerlink" title="TwoViewReconstruction::Reconstruct()"></a>TwoViewReconstruction::Reconstruct()</h3><p>首先，将该类的成员变量初始化，其中vector容器<code>mvKeys1</code>存储参考帧的特征点，容器<code>mvKeys2</code>存储当前帧的特征点。容器mvMatches12是std::vector<Match>类数据，Match是自定义类型，包含两个int型数据。所以<code>mvMatches12</code>存储的是参考帧的特征点索引值和对应的匹配特征点的索引值。容器<code>mvbMatched1</code>存储参考帧中的特征点是否有匹配点。<code>mvSets</code>是个二维向量，每行是8个特征点的索引值，且互不相同，一共200组。<br>接下来，双线程调用FindHomography()和FindFundamental()函数。通过这两个函数得到F和H矩阵后，比较得分，选择得分高的矩阵来还原R和t。</p>
<hr>
<h3 id="TwoViewReconstruction-FindHomography"><a href="#TwoViewReconstruction-FindHomography" class="headerlink" title="TwoViewReconstruction::FindHomography()"></a>TwoViewReconstruction::FindHomography()</h3><p>首先调用函数Normalize()，将坐标归一化，并得到归一化坐标使用的变换矩阵。然后用归一化后的200组点，对每组点用ComputeH21()函数计算出归一化后H矩阵，再用变换矩阵还原H，使用函数CheckHomography()函数对H打分，最后选择得分最高的H。上面调用的函数，具体原理写在最前面了。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/10/ORBSLAM3-NOTE-8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/04/10/ORBSLAM3-NOTE-8/" itemprop="url">ORB-SLAM3源码阅读:ORBmatcher</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2023-04-10T11:34:02+08:00">
                2023-04-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="ORBSLAM3学习笔记-8"><a href="#ORBSLAM3学习笔记-8" class="headerlink" title="ORBSLAM3学习笔记(8)"></a>ORBSLAM3学习笔记(8)</h1><p>这篇博客主要是对ORBmatcher.cc文件的学习。</p>
<hr>
<h2 id="源码阅读"><a href="#源码阅读" class="headerlink" title="源码阅读"></a>源码阅读</h2><h3 id="ORBmatcher-SearchForInitialization"><a href="#ORBmatcher-SearchForInitialization" class="headerlink" title="ORBmatcher::SearchForInitialization()"></a>ORBmatcher::SearchForInitialization()</h3><p><strong>1.创建一个旋转直方图</strong></p>
<pre><code>vector&lt;int&gt; rotHist[HISTO_LENGTH];
// 每个bin里预分配500个，因为使用的是vector不够的话可以自动扩展容量
for(int i=0;i&lt;HISTO_LENGTH;i++)
    rotHist[i].reserve(500);
const float factor = HISTO_LENGTH/360.0f;
</code></pre><p>旋转直方图如下图所示<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/orbslam3/RH.png" alt="旋转直方图"><br>这个直方图就是rotHist，每个柱体就是一个vector容器，称为bin。bin中存储匹配特征点之间的角度差。最后，只取最主流的三个bin，我们只要角度差在这三个范围内的匹配结果，其他的说明特征点角度计算时误差较大，直接舍弃。<br>factor表示每度在整个直方图区间的占比，用来确定角度差的存储在哪个bin里面。</p>
<p><strong>2.在半径窗口内搜索当前帧中所有的候选匹配特征点</strong><br>遍历帧1中的所有特征点(由于这次匹配是用于初始化的，只取金字塔第0层的)</p>
<ul>
<li>在半径窗口内搜索当前帧F2中所有的候选匹配特征点，调用函数GetFeaturesInArea()，具体见<a target="_blank" rel="noopener" href="https://someone-ob.github.io/2023/03/31/ORBSLAM3-NOTE-7/">Frame.cc</a></li>
<li>遍历搜索搜索窗口中的所有潜在的匹配候选点，找到最优的和次优的。匹配的优劣根据描述子的距离判断，两个二进制串之间的汉明距离，指的是其不同位数的个数。</li>
<li>判断最优结果是否满足阈值，判断最佳距离比次佳距离是否小于设定的比例。判断找到的候选特征点对应F1中特征点是否已经匹配过了，如果已经匹配过了，我觉得应该是将这个和原来的匹配都删掉，但源码中是删掉原来的匹配，存入这个新的匹配。如果上面的条件都满足，计算匹配点之间的距离，存入直方图对应的bin中。</li>
</ul>
<p><strong>3.遍历完成，筛除旋转直方图中“非主流”部分</strong></p>
<p><strong>4.将最后通过筛选的匹配好的特征点保存到vbPrevMatched</strong><br>vbPrevMatched是引用参数，将数据保存到该参数中传递回去。</p>
<hr>
<h3 id="ORBmatcher-SearchForTriangulation"><a href="#ORBmatcher-SearchForTriangulation" class="headerlink" title="ORBmatcher::SearchForTriangulation()"></a>ORBmatcher::SearchForTriangulation()</h3><p><strong>1.计算KF1的相机中心在KF2图像平面的二维像素坐标</strong></p>
<blockquote>
<p>直接看我下面的注释就行</p>
</blockquote>
<pre><code>Sophus::SE3f T1w = pKF1-&gt;GetPose();
Sophus::SE3f T2w = pKF2-&gt;GetPose();
Sophus::SE3f Tw2 = pKF2-&gt;GetPoseInverse(); // for convenience
//Cw是关键帧1的光心的世界坐标系下·的坐标
Eigen::Vector3f Cw = pKF1-&gt;GetCameraCenter();
//C2是关键帧1的光心的在关键帧2的坐标系下的坐标
Eigen::Vector3f C2 = T2w * Cw;
//ep是关键帧1的光心在关键帧2上的投影坐标
Eigen::Vector2f ep = pKF2-&gt;mpCamera-&gt;project(C2);
//T12是关键帧2坐标系到关键帧1坐标系的变换矩阵，R12，t12分别存储T12的旋转和平移
Sophus::SE3f T12;
Sophus::SE3f Tll, Tlr, Trl, Trr;
Eigen::Matrix3f R12; // for fastest computation
Eigen::Vector3f t12; // for fastest computation

GeometricCamera* pCamera1 = pKF1-&gt;mpCamera, *pCamera2 = pKF2-&gt;mpCamera;

if(!pKF1-&gt;mpCamera2 &amp;&amp; !pKF2-&gt;mpCamera2)&#123;//单目
    T12 = T1w * Tw2;
    R12 = T12.rotationMatrix();
    t12 = T12.translation();
&#125;
else&#123;//双目
    Sophus::SE3f Tr1w = pKF1-&gt;GetRightPose();
    Sophus::SE3f Twr2 = pKF2-&gt;GetRightPoseInverse();
    Tll = T1w * Tw2;
    Tlr = T1w * Twr2;
    Trl = Tr1w * Tw2;
    Trr = Tr1w * Twr2;
&#125;
Eigen::Matrix3f Rll = Tll.rotationMatrix(), Rlr  = Tlr.rotationMatrix(), Rrl  = Trl.rotationMatrix(), Rrr  = Trr.rotationMatrix();
Eigen::Vector3f tll = Tll.translation(), tlr = Tlr.translation(), trl = Trl.translation(), trr = Trr.translation();

//下面这些参数用处就和上面SearchForInitialization()里面一样了。
int nmatches=0;
// 记录匹配是否成功，避免重复匹配
vector&lt;bool&gt; vbMatched2(pKF2-&gt;N,false);        
vector&lt;int&gt; vMatches12(pKF1-&gt;N,-1);
// 用于统计匹配点对旋转差的直方图
vector&lt;int&gt; rotHist[HISTO_LENGTH];
for(int i=0;i&lt;HISTO_LENGTH;i++)
    rotHist[i].reserve(500);

const float factor = HISTO_LENGTH/360.0f;
</code></pre><p><strong>2.利用BoW加速匹配</strong><br>首先看一下定义的辅助变量</p>
<pre><code>DBoW2::FeatureVector::const_iterator f1it = vFeatVec1.begin();
DBoW2::FeatureVector::const_iterator f2it = vFeatVec2.begin();
DBoW2::FeatureVector::const_iterator f1end = vFeatVec1.end();
DBoW2::FeatureVector::const_iterator f2end = vFeatVec2.end();
</code></pre><p>这里要知道vFeatVec存储了什么，存储的变量是怎么来的，可以看前面推荐的几篇关于词袋方面的博客。</p>
<p>2.1 遍历pKF1和pKF2中的node节点</p>
<blockquote>
<p>首先要进行判断，如果f1it和f2it属于同一个node节点才会进行匹配，这就是BoW加速匹配原理。</p>
</blockquote>
<p>下面讲f1it和f2it属于同一个node节点时该怎么操作</p>
<ol>
<li>遍历该node节点下<strong>f1it</strong>下的所有特征点</li>
<li>通过特征点索引idx1在pKF1中取出对应的MapPoint<blockquote>
<p>若MapPoint存在，则直接进入下一次循环，遍历下一个特征点。若MapPoint不存在，说明该特征点是从未匹配过的。通过这一步骤，我们就能避免重复匹配，导致获得已存在的地图点了。</p>
</blockquote>
</li>
<li>通过特征点索引idx1在pKF1中取出对应的特征点，并取出对应的特征点的描述子。</li>
<li>遍历该node节点下<strong>f2it</strong>对应KF2中的所有特征点</li>
<li>如果pKF2当前特征点索引idx2已经被匹配过或者对应的3d点非空，那么跳过这个索引idx2</li>
<li>通过特征点索引idx2在pKF2中取出对应的特征点和其描述子，计算两个关键帧中对应特征点的描述子距离。</li>
<li><strong>极点ep到kp2的像素距离如果小于阈值th,认为kp2对应的MapPoint距离pKF1相机太近，跳过该匹配点对</strong><blockquote>
<p>作者根据kp2金字塔尺度因子(scale^n，scale=1.2，n为层数)定义阈值th<br> 金字塔层数从0到7，对应距离 sqrt(100*pKF2-&gt;mvScaleFactors[kp2.octave]) 是10-20个像素</p>
</blockquote>
</li>
<li>计算特征点kp2到kp1对应极线的距离是否小于阈值<blockquote>
<p>这个就不细嗦了，这里其实和TwoViewReconstruction.cc那篇博客里的关于F矩阵的卡方检验是一样的。</p>
</blockquote>
</li>
<li>记录匹配结果，记录旋转差直方图信息</li>
</ol>
<p><strong>3.用旋转差直方图来筛掉错误匹配对</strong><br><strong>4.存储匹配关系</strong></p>
<hr>
<h3 id="ORBmatcher-Fuse"><a href="#ORBmatcher-Fuse" class="headerlink" title="ORBmatcher::Fuse()"></a>ORBmatcher::Fuse()</h3><p>该函数的作用是地图点与帧中图像的特征点匹配,实现地图点融合。<br>在将地图点反投影到帧中的过程中,存在以下两种情况:</p>
<ul>
<li>若地图点反投影对应位置上的特征点不存在地图点,说明该特征点之前可能漏掉了，则直接添加观测。</li>
<li>若地图点反投影位置上存在对应地图点,说明两帧中的特征点对应的可能是同一个地图点，但之前出现了误差，则将两个地图点合并到其中观测较多的那个。</li>
</ul>
<p>下面看具体代码：<br>1.获取关键帧（待融合关键帧pKF）位姿、内参、光心在世界坐标系下坐标方便下面使用：Rcw、tcw、fx、fy、cx、cy、bf、Ow。<br>2.遍历所有的待投影地图点，进行如下判断：</p>
<ul>
<li>若该地图点地图点无效 或 已经是该帧的地图点（无需融合），跳过。</li>
<li>将地图点变换到关键帧pKF的相机坐标系下，深度值为负，跳过。</li>
<li>将地图点投影到关键帧pKF的像素坐标系下，若超出了像素坐标系，跳过。</li>
<li>地图点到关键帧相机光心距离需满足在有效范围内，不满足，跳过。</li>
<li>地图点到光心的连线与该地图点的平均观测向量之间夹角要小于60°，不满足跳过。</li>
</ul>
<p>3.在投影点附近搜索窗口内找到候选匹配点的索引</p>
<blockquote>
<p>搜索候选匹配点的索引调用了KeyFrame::GetFeaturesInArea(),这里其实和Frame::GetFeaturesInArea()差不多，具体实现就不细讲了，就讲一下搜索半径怎么来的。<br>首先，根据地图点到光心的距离，得到该点在图像金字塔的层数，然后得到该层的缩放比例，搜索半径就等于th乘以该比例。在orbslam3中，所有th的缺省值都为1。</p>
</blockquote>
<p>4.遍历候选匹配点，寻找最佳匹配点<br>对于每个匹配点，也不是都直接计算描述子距离，首先还要进行判断:</p>
<ul>
<li>金字塔层级要接近（同一层或小一层），否则跳过</li>
<li>计算投影点与候选匹配特征点的距离，进行卡方检验，若超过阈值，直接跳过。</li>
</ul>
<p>若上述判断通过，则计算描述子距离，最后选取距离最小的。<br>5.与最近特征点的描述子距离足够小,就进行地图点融合。</p>
<ul>
<li>地图点反投影位置上存在对应地图点,则将两个地图点合并到其中观测较多的那个则直接添加观测</li>
<li>如果最佳匹配点没有对应地图点，该地图点观测中添加该帧，该帧添加该地图点。</li>
</ul>
<hr>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/31/ORBSLAM3-NOTE-7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/03/31/ORBSLAM3-NOTE-7/" itemprop="url">ORB-SLAM3源码阅读:Frame</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2023-03-31T21:07:04+08:00">
                2023-03-31
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="ORBSLAM3学习笔记-7"><a href="#ORBSLAM3学习笔记-7" class="headerlink" title="ORBSLAM3学习笔记(7)"></a>ORBSLAM3学习笔记(7)</h1><p>这篇博客主要是对Frame.cc文件的学习。</p>
<hr>
<h2 id="源码阅读"><a href="#源码阅读" class="headerlink" title="源码阅读"></a>源码阅读</h2><h3 id="Frame-Frame-缺省构造"><a href="#Frame-Frame-缺省构造" class="headerlink" title="Frame::Frame() 缺省构造"></a>Frame::Frame() 缺省构造</h3><p>把相关变量赋为空或false，无其他操作。</p>
<hr>
<h3 id="Frame-Frame-const-Frame-amp-frame-拷贝构造"><a href="#Frame-Frame-const-Frame-amp-frame-拷贝构造" class="headerlink" title="Frame::Frame(const Frame &amp;frame) 拷贝构造"></a>Frame::Frame(const Frame &amp;frame) 拷贝构造</h3><p>这里就是把参数frame的数据，拷贝到该对象的成员变量中去。</p>
<hr>
<h3 id="Frame-Frame-…-有参数构造"><a href="#Frame-Frame-…-有参数构造" class="headerlink" title="Frame::Frame(…)有参数构造"></a>Frame::Frame(…)有参数构造</h3><p><strong>1.帧id自加</strong></p>
<pre><code>mnId=nNextId++;
</code></pre><p><strong>2.得图像金字塔的参数，如层数、层间比等</strong><br>这些参数都是从传入的ORBextractor类对象中获取。</p>
<p><strong>3.提取orb特征点</strong></p>
<pre><code>ExtractORB(0,imGray,0,1000);
</code></pre><p>这里调用了ExtractORB()函数，第一个参数是左右标志位，单目相机默认是左边。第二个时灰度图，三四是界限，下面具体看该函数</p>
<pre><code>void Frame::ExtractORB(int flag, const cv::Mat &amp;im, const int x0, const int x1)
&#123;
    vector&lt;int&gt; vLapping = &#123;x0,x1&#125;;
    // 判断是左图还是右图
    if(flag==0)
        // 左图的话就套使用左图指定的特征点提取器，并将提取结果保存到对应的变量中 
        monoLeft = (*mpORBextractorLeft)(im,cv::Mat(),mvKeys,mDescriptors,vLapping);
    else
        // 右图的话就需要使用右图指定的特征点提取器，并将提取结果保存到对应的变量中 
        monoRight = (*mpORBextractorRight)(im,cv::Mat(),mvKeysRight,mDescriptorsRight,vLapping);
&#125;
</code></pre><p>(*mpORBextractorLeft)(im,cv::Mat(),mvKeys,mDescriptors,vLapping);其实就是调用了ORBextractor类的()的重载函数。具体内容可以看<a target="_blank" rel="noopener" href="https://someone-ob.github.io/2023/03/30/ORBSLAM3-NOTE-4/">ORBextractor.cc</a></p>
<p><strong>4.畸变矫正</strong><br>这里就是先把mvKeys存储的特征点的坐标放到一个Mat矩阵里，用opencv库里的函数去其去畸变，把得到的新的坐标覆盖掉mvKeys里的旧数据。去畸变是因为之后要用这些特征点匹配计算位姿变换，我们需要路标点投影到相机平面的正确的坐标。</p>
<pre><code>UndistortKeyPoints();
</code></pre><p>畸变矫正完，由于单目相机无法直接获得立体信息，所以这里要给右图像对应点和深度赋值-1表示没有相关信息</p>
<pre><code>mvuRight = vector&lt;float&gt;(N,-1);
mvDepth = vector&lt;float&gt;(N,-1);
mnCloseMPs = 0;
</code></pre><p><strong>5.初始化本帧的地图点</strong></p>
<pre><code>mvpMapPoints = vector&lt;MapPoint*&gt;(N,static_cast&lt;MapPoint*&gt;(NULL));

mmProjectPoints.clear();// = map&lt;long unsigned int, cv::Point2f&gt;(N, static_cast&lt;cv::Point2f&gt;(NULL));
mmMatchedInImage.clear();
</code></pre><p>这里就是把vector容器的大小初始化了一下，里面没有内容。</p>
<p><strong>6.计算去畸变后图像边界，将特征点分配到网格中。</strong><br>存放特征点的网格mGrid是一个二维数组，数组中的每个元素都是一个vector容器。这个网格是在用来做特征点匹配的，因为暴力匹配的效率实在是有点低，下面看步骤：</p>
<ul>
<li>若图像是第一帧或者是相机标定参数发生变化，则<ul>
<li>计算去畸变后图像的边界，这里也是用opencv库里的函数，这步将mnMinX，mnMaxX，mnMinY，mnMaxY初始化。</li>
<li>计算一个图像像素在网格中占的宽和高mfGridElementWidthInv，mfGridElementHeightInv，这两个变量用来确定特征点在哪个网格。</li>
<li>特殊的初始化过程完成，标志复位mbInitialComputations=false;</li>
</ul>
</li>
<li>设置一些非立体鱼眼模式的标志位  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Nleft = -1;</span><br><span class="line">Nright = -1;</span><br><span class="line">mvLeftToRightMatch = vector&lt;int&gt;(0);</span><br><span class="line">mvRightToLeftMatch = vector&lt;int&gt;(0);</span><br><span class="line">mvStereo3Dpoints = vector&lt;Eigen::Vector3f&gt;(0);</span><br><span class="line">monoLeft = -1;</span><br><span class="line">monoRight = -1;</span><br></pre></td></tr></table></figure></li>
<li>将特征点分配到图像网格中 ，调用函数AssignFeaturesToGrid()</li>
</ul>
<hr>
<h3 id="Frame-AssignFeaturesToGrid"><a href="#Frame-AssignFeaturesToGrid" class="headerlink" title="Frame::AssignFeaturesToGrid()"></a>Frame::AssignFeaturesToGrid()</h3><p><strong>1.给存储特征点的网格数组 Frame::mGrid 预分配空间</strong></p>
<pre><code>const int nCells = FRAME_GRID_COLS*FRAME_GRID_ROWS;

int nReserve = 0.5f*N/(nCells);

// 开始对mGrid这个二维数组中的每一个vector元素遍历并预分配空间
for(unsigned int i=0; i&lt;FRAME_GRID_COLS;i++)
    for (unsigned int j=0; j&lt;FRAME_GRID_ROWS;j++)&#123;
        mGrid[i][j].reserve(nReserve);
        if(Nleft != -1)&#123;
            mGridRight[i][j].reserve(nReserve);
        &#125;
    &#125;
</code></pre><p>这里nReserve定义为0.5f*N/(nCells)，乘以0.5是为了避免内存空间的浪费，反正vector能动态扩容，如果有些格子里的特征点特别多，原来预分配的不够用了，动态申请内存空间就行。</p>
<p><strong>2.遍历每个特征点，将每个特征点在mvKeysUn中的索引值放到对应的网格mGrid中</strong></p>
<pre><code>for(int i=0;i&lt;N;i++)
&#123;
    const cv::KeyPoint &amp;kp = (Nleft == -1) ? mvKeysUn[i]
                                             : (i &lt; Nleft) ? mvKeys[i]
                                                             : mvKeysRight[i - Nleft];
    // 存储某个特征点所在网格的网格坐标，nGridPosX范围：[0,FRAME_GRID_COLS], nGridPosY范围：[0,FRAME_GRID_ROWS]
    int nGridPosX, nGridPosY;
    // 计算某个特征点所在网格的网格坐标，如果找到特征点所在的网格坐标，记录在nGridPosX,nGridPosY里，返回true，没找到返回false
    if(PosInGrid(kp,nGridPosX,nGridPosY))&#123;
        if(Nleft == -1 || i &lt; Nleft)
            // 如果找到特征点所在网格坐标，将这个特征点的索引添加到对应网格的数组mGrid中
            mGrid[nGridPosX][nGridPosY].push_back(i);
        else
            mGridRight[nGridPosX][nGridPosY].push_back(i - Nleft);
    &#125;
&#125;
</code></pre><p>遍历每个关键点，调用PosInGrid()函数，计算出每个关键点在哪个网格，然后将其索引值放入对应网格的vector当中。</p>
<hr>
<h3 id="Frame-GetFeaturesInArea"><a href="#Frame-GetFeaturesInArea" class="headerlink" title="Frame::GetFeaturesInArea()"></a>Frame::GetFeaturesInArea()</h3><p>该函数用来找候选的匹配点，简单来说就是根据需要匹配的点坐标(x,y)和半径r确定边界，遍历在边界内的网格，确认网格内的特征点是第0层的，且在圆内，则把它们push到vector容器中。<br><img src="https://cdn.jsdelivr.net/gh/someone-ob/ImgHosting/PIC/orbslam3/getfeatures.png" alt="获取候选特征点"></p>
<hr>
<h3 id="Frame-isInFrustum"><a href="#Frame-isInFrustum" class="headerlink" title="Frame::isInFrustum()"></a>Frame::isInFrustum()</h3><p>1.获得这个地图点的世界坐标<br>2.检查这个地图点在当前帧的相机坐标系下，是否有正的深度.如果是负的，表示出错，返回false<br>3.将MapPoint投影到当前帧的像素坐标(u,v), 并判断是否在图像有效范围内<br>4.计算MapPoint到相机中心的距离, 并判断是否在尺度变化的距离内</p>
<blockquote>
<p>这里的尺度是0.8 <em> mfMinDistance到1.2 </em> mfMaxDistance，mfMinDistance和mfMaxDistance在Mappoint文件里关于观测距离范围那部分有讲。</p>
</blockquote>
<p>5.计算当前相机指向地图点向量和地图点的平均观测方向夹角的余弦值, 若小于设定阈值，返回false<br>6.根据地图点到光心的距离来预测一个尺度</p>
<blockquote>
<p>这里的尺度是指地图点对应的金字塔层数，公式是$\text{m}=ceil\left( \frac{\log \left( \frac{d_{\max} }{d} \right)}{\log \left( 1.2 \right)} \right)$,可以参考Mappoint.cc里关于观测距离范围那部分。</p>
</blockquote>
<p>7.记录计算得到的一些参数</p>
<pre><code>pMP-&gt;mbTrackInView = true;
// 该地图点投影在当前图像（一般是左图）的像素横坐标
pMP-&gt;mTrackProjX = uv(0);
// bf/z其实是视差，相减得到右图（如有）中对应点的横坐标
pMP-&gt;mTrackProjXR = uv(0) - mbf*invz;

pMP-&gt;mTrackDepth = Pc_dist;

// 该地图点投影在当前图像（一般是左图）的像素纵坐标
pMP-&gt;mTrackProjY = uv(1);
// 根据地图点到光心距离，预测的该地图点的尺度层级
pMP-&gt;mnTrackScaleLevel= nPredictedLevel;
// 保存当前视角和法线夹角的余弦值
pMP-&gt;mTrackViewCos = viewCos;
</code></pre>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">&gt;</a>
  </nav>


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
